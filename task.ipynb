{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "debc1289-19a6-418f-963d-97ce38fdb7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "|      Date|      Open|      High|       Low|     Close|   Volume|Adj Close|\n",
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "|2010-01-04|213.429998|214.499996|212.380001|214.009998|123432400|27.727039|\n",
      "|2010-01-05|214.599998|215.589994|213.249994|214.379993|150476200|27.774976|\n",
      "|2010-01-06|214.379993|    215.23|210.750004|210.969995|138040000|27.333178|\n",
      "|2010-01-07|    211.75|212.000006|209.050005|    210.58|119282800| 27.28265|\n",
      "|2010-01-08|210.299994|212.000006|209.060005|211.980005|111902700|27.464034|\n",
      "|2010-01-11|212.799997|213.000002|208.450005|210.110003|115557400|27.221758|\n",
      "|2010-01-12|209.189995|209.769995|206.419998|207.720001|148614900| 26.91211|\n",
      "|2010-01-13|207.870005|210.929995|204.099998|210.650002|151473000| 27.29172|\n",
      "|2010-01-14|210.110003|210.459997|209.020004|    209.43|108223500|27.133657|\n",
      "|2010-01-15|210.929995|211.599997|205.869999|    205.93|148516900|26.680198|\n",
      "|2010-01-19|208.330002|215.189999|207.240004|215.039995|182501900|27.860485|\n",
      "|2010-01-20|214.910006|215.549994|209.500002|    211.73|153038200|27.431644|\n",
      "|2010-01-21|212.079994|213.309996|207.210003|208.069996|152038600|26.957455|\n",
      "|2010-01-22|206.780006|207.499996|    197.16|    197.75|220441900|25.620401|\n",
      "|2010-01-25|202.510002|204.699999|200.190002|203.070002|266424900|26.309658|\n",
      "|2010-01-26|205.950001|213.710005|202.580004|205.940001|466777500|26.681494|\n",
      "|2010-01-27|206.849995|    210.58|199.530001|207.880005|430642100| 26.93284|\n",
      "|2010-01-28|204.930004|205.500004|198.699995|199.289995|293375600|25.819922|\n",
      "|2010-01-29|201.079996|202.199995|190.250002|192.060003|311488100|24.883208|\n",
      "|2010-02-01|192.369997|     196.0|191.299999|194.729998|187469100|25.229131|\n",
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# task1\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('myapli').getOrCreate()\n",
    "df=spark.read.csv('Tasks - Apple stock.csv',header=True,inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fef67f4-7cae-4234-9bdb-ef3e136379c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# task2\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('myapli').getOrCreate()\n",
    "df=spark.read.csv('Tasks - Apple stock.csv')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "221ecaf9-6657-4b1d-8eeb-3ccad836b5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "|      Date|      Open|      High|       Low|     Close|   Volume|Adj Close|\n",
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "|2010-01-04|213.429998|214.499996|212.380001|214.009998|123432400|27.727039|\n",
      "|2010-01-05|214.599998|215.589994|213.249994|214.379993|150476200|27.774976|\n",
      "|2010-01-06|214.379993|    215.23|210.750004|210.969995|138040000|27.333178|\n",
      "|2010-01-07|    211.75|212.000006|209.050005|    210.58|119282800| 27.28265|\n",
      "|2010-01-08|210.299994|212.000006|209.060005|211.980005|111902700|27.464034|\n",
      "|2010-01-11|212.799997|213.000002|208.450005|210.110003|115557400|27.221758|\n",
      "|2010-01-12|209.189995|209.769995|206.419998|207.720001|148614900| 26.91211|\n",
      "|2010-01-13|207.870005|210.929995|204.099998|210.650002|151473000| 27.29172|\n",
      "|2010-01-14|210.110003|210.459997|209.020004|    209.43|108223500|27.133657|\n",
      "|2010-01-15|210.929995|211.599997|205.869999|    205.93|148516900|26.680198|\n",
      "|2010-01-19|208.330002|215.189999|207.240004|215.039995|182501900|27.860485|\n",
      "|2010-01-20|214.910006|215.549994|209.500002|    211.73|153038200|27.431644|\n",
      "|2010-01-21|212.079994|213.309996|207.210003|208.069996|152038600|26.957455|\n",
      "|2010-01-22|206.780006|207.499996|    197.16|    197.75|220441900|25.620401|\n",
      "|2010-01-25|202.510002|204.699999|200.190002|203.070002|266424900|26.309658|\n",
      "|2010-01-26|205.950001|213.710005|202.580004|205.940001|466777500|26.681494|\n",
      "|2010-01-27|206.849995|    210.58|199.530001|207.880005|430642100| 26.93284|\n",
      "|2010-01-28|204.930004|205.500004|198.699995|199.289995|293375600|25.819922|\n",
      "|2010-01-29|201.079996|202.199995|190.250002|192.060003|311488100|24.883208|\n",
      "|2010-02-01|192.369997|     196.0|191.299999|194.729998|187469100|25.229131|\n",
      "|2010-02-02|195.909998|196.319994|193.379993|195.859997|174585600|25.375533|\n",
      "|2010-02-03|195.169994|200.200003|194.420004|199.229994|153832000|25.812149|\n",
      "|2010-02-04|196.730003|198.370001|191.570005|192.050003|189413000|24.881912|\n",
      "|2010-02-05|192.630003|     196.0|190.850002|195.460001|212576700| 25.32371|\n",
      "|2010-02-08|195.690006|197.880003|193.999994|194.119997|119567700|  25.1501|\n",
      "|2010-02-09|196.419996|197.499994|194.749998|196.190004|158221700|25.418289|\n",
      "|2010-02-10|195.889997|     196.6|    194.26|195.120007| 92590400| 25.27966|\n",
      "|2010-02-11|194.880001|199.750006|194.059996|198.669994|137586400|25.739595|\n",
      "|2010-02-12|198.109995|201.639996|195.500002|200.379993|163867200|25.961142|\n",
      "|2010-02-16|201.940002|203.690002|201.520006|203.399996|135934400|26.352412|\n",
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#task3\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('myapli').getOrCreate()\n",
    "df=spark.read.csv('Tasks - Apple stock.csv',header=True,inferSchema=True)\n",
    "df.show(30)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f83efb9f-f4c6-4a0c-b1e7-f0343cd8026c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column name: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']\n",
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "|      Date|      Open|      High|       Low|     Close|   Volume|Adj Close|\n",
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "|2010-01-04|213.429998|214.499996|212.380001|214.009998|123432400|27.727039|\n",
      "|2010-01-05|214.599998|215.589994|213.249994|214.379993|150476200|27.774976|\n",
      "|2010-01-06|214.379993|    215.23|210.750004|210.969995|138040000|27.333178|\n",
      "|2010-01-07|    211.75|212.000006|209.050005|    210.58|119282800| 27.28265|\n",
      "|2010-01-08|210.299994|212.000006|209.060005|211.980005|111902700|27.464034|\n",
      "|2010-01-11|212.799997|213.000002|208.450005|210.110003|115557400|27.221758|\n",
      "|2010-01-12|209.189995|209.769995|206.419998|207.720001|148614900| 26.91211|\n",
      "|2010-01-13|207.870005|210.929995|204.099998|210.650002|151473000| 27.29172|\n",
      "|2010-01-14|210.110003|210.459997|209.020004|    209.43|108223500|27.133657|\n",
      "|2010-01-15|210.929995|211.599997|205.869999|    205.93|148516900|26.680198|\n",
      "|2010-01-19|208.330002|215.189999|207.240004|215.039995|182501900|27.860485|\n",
      "|2010-01-20|214.910006|215.549994|209.500002|    211.73|153038200|27.431644|\n",
      "|2010-01-21|212.079994|213.309996|207.210003|208.069996|152038600|26.957455|\n",
      "|2010-01-22|206.780006|207.499996|    197.16|    197.75|220441900|25.620401|\n",
      "|2010-01-25|202.510002|204.699999|200.190002|203.070002|266424900|26.309658|\n",
      "|2010-01-26|205.950001|213.710005|202.580004|205.940001|466777500|26.681494|\n",
      "|2010-01-27|206.849995|    210.58|199.530001|207.880005|430642100| 26.93284|\n",
      "|2010-01-28|204.930004|205.500004|198.699995|199.289995|293375600|25.819922|\n",
      "|2010-01-29|201.079996|202.199995|190.250002|192.060003|311488100|24.883208|\n",
      "|2010-02-01|192.369997|       196|191.299999|194.729998|187469100|25.229131|\n",
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# task4\n",
    "from pyspark.sql import  SparkSession\n",
    "spark=SparkSession.builder.appName('myapli').getOrCreate()\n",
    "df=spark.read.csv('Tasks - Apple stock.csv',header=True)\n",
    "print('column name:',df.columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a864938c-e9f5-4ffa-975b-b9ad1f2b4bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      Date|      open|\n",
      "+----------+----------+\n",
      "|2010-01-04|213.429998|\n",
      "|2010-01-05|214.599998|\n",
      "|2010-01-06|214.379993|\n",
      "|2010-01-07|    211.75|\n",
      "|2010-01-08|210.299994|\n",
      "|2010-01-11|212.799997|\n",
      "|2010-01-12|209.189995|\n",
      "|2010-01-13|207.870005|\n",
      "|2010-01-14|210.110003|\n",
      "|2010-01-15|210.929995|\n",
      "|2010-01-19|208.330002|\n",
      "|2010-01-20|214.910006|\n",
      "|2010-01-21|212.079994|\n",
      "|2010-01-22|206.780006|\n",
      "|2010-01-25|202.510002|\n",
      "|2010-01-26|205.950001|\n",
      "|2010-01-27|206.849995|\n",
      "|2010-01-28|204.930004|\n",
      "|2010-01-29|201.079996|\n",
      "|2010-02-01|192.369997|\n",
      "+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# task5\n",
    "from pyspark.sql import  SparkSession\n",
    "spark=SparkSession.builder.appName('myapli').getOrCreate()\n",
    "df=spark.read.csv('Tasks - Apple stock.csv',header=True)\n",
    "res=df.select('Date','open')\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31987046-00e2-49e3-a496-5909bc6a62a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "|      Date|      Open|      High|       Low|     Close|   Volume|Adj Close|\n",
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "|2010-01-04|213.429998|214.499996|212.380001|214.009998|123432400|27.727039|\n",
      "|2010-01-05|214.599998|215.589994|213.249994|214.379993|150476200|27.774976|\n",
      "|2010-01-06|214.379993|    215.23|210.750004|210.969995|138040000|27.333178|\n",
      "|2010-01-07|    211.75|212.000006|209.050005|    210.58|119282800| 27.28265|\n",
      "|2010-01-08|210.299994|212.000006|209.060005|211.980005|111902700|27.464034|\n",
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# task6\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('myapli').getOrCreate()\n",
    "df=spark.read.csv('Tasks - Apple stock.csv',header=True)\n",
    "res=df.limit(5)\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3df484de-6cc9-491a-b45a-192e74df3a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the first row:\n",
      "Row(Date='2010-01-04', Open='213.429998', High='214.499996', Low='212.380001', Close='214.009998', Volume='123432400', Adj Close='27.727039')\n",
      "\n",
      "THE FIRST 5 ROWS:\n",
      "Row(Date='2010-01-04', Open='213.429998', High='214.499996', Low='212.380001', Close='214.009998', Volume='123432400', Adj Close='27.727039')\n",
      "Row(Date='2010-01-05', Open='214.599998', High='215.589994', Low='213.249994', Close='214.379993', Volume='150476200', Adj Close='27.774976')\n",
      "Row(Date='2010-01-06', Open='214.379993', High='215.23', Low='210.750004', Close='210.969995', Volume='138040000', Adj Close='27.333178')\n",
      "Row(Date='2010-01-07', Open='211.75', High='212.000006', Low='209.050005', Close='210.58', Volume='119282800', Adj Close='27.28265')\n",
      "Row(Date='2010-01-08', Open='210.299994', High='212.000006', Low='209.060005', Close='211.980005', Volume='111902700', Adj Close='27.464034')\n",
      "\n",
      "{'Date': '2010-01-04', 'Open': '213.429998', 'High': '214.499996', 'Low': '212.380001', 'Close': '214.009998', 'Volume': '123432400', 'Adj Close': '27.727039'}\n",
      "{'Date': '2010-01-05', 'Open': '214.599998', 'High': '215.589994', 'Low': '213.249994', 'Close': '214.379993', 'Volume': '150476200', 'Adj Close': '27.774976'}\n",
      "{'Date': '2010-01-06', 'Open': '214.379993', 'High': '215.23', 'Low': '210.750004', 'Close': '210.969995', 'Volume': '138040000', 'Adj Close': '27.333178'}\n",
      "{'Date': '2010-01-07', 'Open': '211.75', 'High': '212.000006', 'Low': '209.050005', 'Close': '210.58', 'Volume': '119282800', 'Adj Close': '27.28265'}\n",
      "{'Date': '2010-01-08', 'Open': '210.299994', 'High': '212.000006', 'Low': '209.060005', 'Close': '211.980005', 'Volume': '111902700', 'Adj Close': '27.464034'}\n"
     ]
    }
   ],
   "source": [
    "# task7\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('myapli').getOrCreate()\n",
    "df=spark.read.csv('Tasks - Apple stock.csv',header=True)\n",
    "row=df.head()\n",
    "print('the first row:')\n",
    "print(row)\n",
    "print()\n",
    "\n",
    "res=df.head(5)\n",
    "print('THE FIRST 5 ROWS:')\n",
    "for i in res:\n",
    "    print(i)\n",
    "print()\n",
    "# dict=res[0].asDict()\n",
    "# print('the dict of of first rows:')\n",
    "# print(dict)\n",
    "\n",
    "for i in res:\n",
    "    dict=i.asDict()\n",
    "    print(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fbb18a41-3a49-42f1-ab29-6327325392e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1762\n"
     ]
    }
   ],
   "source": [
    "# task8\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('myapli').getOrCreate()\n",
    "df=spark.read.csv('Tasks - Apple stock.csv',header=True)\n",
    "res=df.count()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d8d10ada-26d6-4b0b-9e90-2c62147fce47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      open|\n",
      "+----------+\n",
      "|213.429998|\n",
      "|214.599998|\n",
      "|214.379993|\n",
      "|    211.75|\n",
      "|210.299994|\n",
      "|212.799997|\n",
      "|209.189995|\n",
      "|207.870005|\n",
      "|210.110003|\n",
      "|210.929995|\n",
      "|208.330002|\n",
      "|214.910006|\n",
      "|212.079994|\n",
      "|206.780006|\n",
      "|202.510002|\n",
      "|205.950001|\n",
      "|206.849995|\n",
      "|204.930004|\n",
      "|201.079996|\n",
      "|192.369997|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "\n",
      "+----------+\n",
      "|      open|\n",
      "+----------+\n",
      "|     118.5|\n",
      "|134.460007|\n",
      "|120.800003|\n",
      "|248.540001|\n",
      "|232.819988|\n",
      "| 318.94001|\n",
      "|491.650024|\n",
      "|     90.82|\n",
      "|108.860001|\n",
      "|267.269989|\n",
      "| 598.37001|\n",
      "|540.420006|\n",
      "| 528.94001|\n",
      "|542.379997|\n",
      "|116.550003|\n",
      "|117.879997|\n",
      "|111.599998|\n",
      "|397.029995|\n",
      "|553.400009|\n",
      "|519.610023|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# task9\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('myapli').getOrCreate()\n",
    "df=spark.read.csv('Tasks - Apple stock.csv',header=True)\n",
    "res=df.select('open')\n",
    "print(res.show())\n",
    "print()\n",
    "res1=res.distinct()\n",
    "res1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6668ea26-1b20-4bec-ba30-038d70ad74e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      open|\n",
      "+----------+\n",
      "|213.429998|\n",
      "|214.599998|\n",
      "|214.379993|\n",
      "|    211.75|\n",
      "|210.299994|\n",
      "|212.799997|\n",
      "|209.189995|\n",
      "|207.870005|\n",
      "|210.110003|\n",
      "|210.929995|\n",
      "|208.330002|\n",
      "|214.910006|\n",
      "|212.079994|\n",
      "|206.780006|\n",
      "|202.510002|\n",
      "|205.950001|\n",
      "|206.849995|\n",
      "|204.930004|\n",
      "|201.079996|\n",
      "|192.369997|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "+----------+\n",
      "|      open|\n",
      "+----------+\n",
      "|     118.5|\n",
      "|134.460007|\n",
      "|120.800003|\n",
      "|248.540001|\n",
      "|232.819988|\n",
      "| 318.94001|\n",
      "|491.650024|\n",
      "|     90.82|\n",
      "|108.860001|\n",
      "|267.269989|\n",
      "| 598.37001|\n",
      "|540.420006|\n",
      "| 528.94001|\n",
      "|542.379997|\n",
      "|116.550003|\n",
      "|117.879997|\n",
      "|111.599998|\n",
      "|397.029995|\n",
      "|553.400009|\n",
      "|519.610023|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1685"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# task10\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('myapli').getOrCreate()\n",
    "df=spark.read.csv('Tasks - Apple stock.csv',header=True)\n",
    "res=df.select('open')\n",
    "res.show()\n",
    "print()\n",
    "d=res.distinct()\n",
    "d.show()\n",
    "d.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a66d420e-d75e-4031-a273-8593c15ef139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+------------------+----------+---------+---------+\n",
      "|      Date|      Open|      High|       Low|             Range|     Close|   Volume|Adj Close|\n",
      "+----------+----------+----------+----------+------------------+----------+---------+---------+\n",
      "|2010-01-04|213.429998|214.499996|212.380001| 2.119995000000017|214.009998|123432400|27.727039|\n",
      "|2010-01-05|214.599998|215.589994|213.249994|2.3400000000000034|214.379993|150476200|27.774976|\n",
      "|2010-01-06|214.379993|    215.23|210.750004|          4.479996|210.969995|138040000|27.333178|\n",
      "|2010-01-07|    211.75|212.000006|209.050005|2.9500010000000145|    210.58|119282800| 27.28265|\n",
      "|2010-01-08|210.299994|212.000006|209.060005|2.9400010000000236|211.980005|111902700|27.464034|\n",
      "|2010-01-11|212.799997|213.000002|208.450005|4.5499969999999905|210.110003|115557400|27.221758|\n",
      "|2010-01-12|209.189995|209.769995|206.419998| 3.349997000000002|207.720001|148614900| 26.91211|\n",
      "|2010-01-13|207.870005|210.929995|204.099998| 6.829996999999992|210.650002|151473000| 27.29172|\n",
      "|2010-01-14|210.110003|210.459997|209.020004| 1.439992999999987|    209.43|108223500|27.133657|\n",
      "|2010-01-15|210.929995|211.599997|205.869999| 5.729997999999995|    205.93|148516900|26.680198|\n",
      "|2010-01-19|208.330002|215.189999|207.240004| 7.949995000000001|215.039995|182501900|27.860485|\n",
      "|2010-01-20|214.910006|215.549994|209.500002| 6.049992000000003|    211.73|153038200|27.431644|\n",
      "|2010-01-21|212.079994|213.309996|207.210003| 6.099993000000012|208.069996|152038600|26.957455|\n",
      "|2010-01-22|206.780006|207.499996|    197.16|10.339996000000014|    197.75|220441900|25.620401|\n",
      "|2010-01-25|202.510002|204.699999|200.190002|4.5099969999999985|203.070002|266424900|26.309658|\n",
      "|2010-01-26|205.950001|213.710005|202.580004|11.130000999999993|205.940001|466777500|26.681494|\n",
      "|2010-01-27|206.849995|    210.58|199.530001|11.049999000000014|207.880005|430642100| 26.93284|\n",
      "|2010-01-28|204.930004|205.500004|198.699995| 6.800008999999989|199.289995|293375600|25.819922|\n",
      "|2010-01-29|201.079996|202.199995|190.250002|11.949993000000006|192.060003|311488100|24.883208|\n",
      "|2010-02-01|192.369997|     196.0|191.299999| 4.700000999999986|194.729998|187469100|25.229131|\n",
      "+----------+----------+----------+----------+------------------+----------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# task11\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CSVExample\").getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"Tasks - Apple stock.csv\", header=True, inferSchema=True)\n",
    "\n",
    "range_column = col(\"High\") - col(\"Low\")\n",
    "\n",
    "df = df.withColumn(\"Range\", range_column)\n",
    "\n",
    "df = df.select(\n",
    "    \"Date\", \"Open\", \"High\", \"Low\", \"Range\", \"Close\", \"Volume\", \"Adj Close\"\n",
    ")\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cf6c6e8b-fac1-4c41-95c9-1f0ed9b8d185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+------------------+----------+---------+---------+\n",
      "|      Date|      Open|      High|       Low|             Range|     Close|   Volume|Adj Close|\n",
      "+----------+----------+----------+----------+------------------+----------+---------+---------+\n",
      "|2010-01-04|213.429998|214.499996|212.380001| 2.119995000000017|214.009998|123432400|27.727039|\n",
      "|2010-01-05|214.599998|215.589994|213.249994|2.3400000000000034|214.379993|150476200|27.774976|\n",
      "|2010-01-06|214.379993|    215.23|210.750004|          4.479996|210.969995|138040000|27.333178|\n",
      "|2010-01-07|    211.75|212.000006|209.050005|2.9500010000000145|    210.58|119282800| 27.28265|\n",
      "|2010-01-08|210.299994|212.000006|209.060005|2.9400010000000236|211.980005|111902700|27.464034|\n",
      "|2010-01-11|212.799997|213.000002|208.450005|4.5499969999999905|210.110003|115557400|27.221758|\n",
      "|2010-01-12|209.189995|209.769995|206.419998| 3.349997000000002|207.720001|148614900| 26.91211|\n",
      "|2010-01-13|207.870005|210.929995|204.099998| 6.829996999999992|210.650002|151473000| 27.29172|\n",
      "|2010-01-14|210.110003|210.459997|209.020004| 1.439992999999987|    209.43|108223500|27.133657|\n",
      "|2010-01-15|210.929995|211.599997|205.869999| 5.729997999999995|    205.93|148516900|26.680198|\n",
      "|2010-01-19|208.330002|215.189999|207.240004| 7.949995000000001|215.039995|182501900|27.860485|\n",
      "|2010-01-20|214.910006|215.549994|209.500002| 6.049992000000003|    211.73|153038200|27.431644|\n",
      "|2010-01-21|212.079994|213.309996|207.210003| 6.099993000000012|208.069996|152038600|26.957455|\n",
      "|2010-01-22|206.780006|207.499996|    197.16|10.339996000000014|    197.75|220441900|25.620401|\n",
      "|2010-01-25|202.510002|204.699999|200.190002|4.5099969999999985|203.070002|266424900|26.309658|\n",
      "|2010-01-26|205.950001|213.710005|202.580004|11.130000999999993|205.940001|466777500|26.681494|\n",
      "|2010-01-27|206.849995|    210.58|199.530001|11.049999000000014|207.880005|430642100| 26.93284|\n",
      "|2010-01-28|204.930004|205.500004|198.699995| 6.800008999999989|199.289995|293375600|25.819922|\n",
      "|2010-01-29|201.079996|202.199995|190.250002|11.949993000000006|192.060003|311488100|24.883208|\n",
      "|2010-02-01|192.369997|     196.0|191.299999| 4.700000999999986|194.729998|187469100|25.229131|\n",
      "+----------+----------+----------+----------+------------------+----------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+----------+----------+----------+------------------+----------+---------+---------+\n",
      "|      Date|      Open|      High|       Low|             Range|     Close|   Volume|Adj Close|\n",
      "+----------+----------+----------+----------+------------------+----------+---------+---------+\n",
      "|2010-01-04|213.429998|214.499996|212.380001| 2.119995000000017|214.009998|123432400|27.727039|\n",
      "|2010-01-05|214.599998|215.589994|213.249994|2.3400000000000034|214.379993|150476200|27.774976|\n",
      "|2010-01-06|214.379993|    215.23|210.750004|          4.479996|210.969995|138040000|27.333178|\n",
      "|2010-01-07|    211.75|212.000006|209.050005|2.9500010000000145|    210.58|119282800| 27.28265|\n",
      "|2010-01-08|210.299994|212.000006|209.060005|2.9400010000000236|211.980005|111902700|27.464034|\n",
      "|2010-01-11|212.799997|213.000002|208.450005|4.5499969999999905|210.110003|115557400|27.221758|\n",
      "|2010-01-12|209.189995|209.769995|206.419998| 3.349997000000002|207.720001|148614900| 26.91211|\n",
      "|2010-01-13|207.870005|210.929995|204.099998| 6.829996999999992|210.650002|151473000| 27.29172|\n",
      "|2010-01-14|210.110003|210.459997|209.020004| 1.439992999999987|    209.43|108223500|27.133657|\n",
      "|2010-01-15|210.929995|211.599997|205.869999| 5.729997999999995|    205.93|148516900|26.680198|\n",
      "|2010-01-19|208.330002|215.189999|207.240004| 7.949995000000001|215.039995|182501900|27.860485|\n",
      "|2010-01-20|214.910006|215.549994|209.500002| 6.049992000000003|    211.73|153038200|27.431644|\n",
      "|2010-01-21|212.079994|213.309996|207.210003| 6.099993000000012|208.069996|152038600|26.957455|\n",
      "|2010-01-22|206.780006|207.499996|    197.16|10.339996000000014|    197.75|220441900|25.620401|\n",
      "|2010-01-25|202.510002|204.699999|200.190002|4.5099969999999985|203.070002|266424900|26.309658|\n",
      "|2010-01-26|205.950001|213.710005|202.580004|11.130000999999993|205.940001|466777500|26.681494|\n",
      "|2010-01-27|206.849995|    210.58|199.530001|11.049999000000014|207.880005|430642100| 26.93284|\n",
      "|2010-01-28|204.930004|205.500004|198.699995| 6.800008999999989|199.289995|293375600|25.819922|\n",
      "|2010-01-29|201.079996|202.199995|190.250002|11.949993000000006|192.060003|311488100|24.883208|\n",
      "|2010-02-01|192.369997|     196.0|191.299999| 4.700000999999986|194.729998|187469100|25.229131|\n",
      "+----------+----------+----------+----------+------------------+----------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "spark=SparkSession.builder.appName('myapli').getOrCreate()\n",
    "df.show()\n",
    "res=df.select('Date','Open','High','Low',(col('High')-col('Low')).alias('Range'),'Close','Volume','Adj Close')\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "36a506b8-4295-4d94-96fa-620c05b2cec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "|      Date|      Open|      High|       Low|     Close|   Volume|Adj Close|\n",
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "|2010-01-04|213.429998|214.499996|212.380001|214.009998|123432400|27.727039|\n",
      "|2010-01-05|214.599998|215.589994|213.249994|214.379993|150476200|27.774976|\n",
      "|2010-01-06|214.379993|    215.23|210.750004|210.969995|138040000|27.333178|\n",
      "|2010-01-07|    211.75|212.000006|209.050005|    210.58|119282800| 27.28265|\n",
      "|2010-01-08|210.299994|212.000006|209.060005|211.980005|111902700|27.464034|\n",
      "|2010-01-11|212.799997|213.000002|208.450005|210.110003|115557400|27.221758|\n",
      "|2010-01-12|209.189995|209.769995|206.419998|207.720001|148614900| 26.91211|\n",
      "|2010-01-13|207.870005|210.929995|204.099998|210.650002|151473000| 27.29172|\n",
      "|2010-01-14|210.110003|210.459997|209.020004|    209.43|108223500|27.133657|\n",
      "|2010-01-15|210.929995|211.599997|205.869999|    205.93|148516900|26.680198|\n",
      "|2010-01-19|208.330002|215.189999|207.240004|215.039995|182501900|27.860485|\n",
      "|2010-01-20|214.910006|215.549994|209.500002|    211.73|153038200|27.431644|\n",
      "|2010-01-21|212.079994|213.309996|207.210003|208.069996|152038600|26.957455|\n",
      "|2010-01-22|206.780006|207.499996|    197.16|    197.75|220441900|25.620401|\n",
      "|2010-01-25|202.510002|204.699999|200.190002|203.070002|266424900|26.309658|\n",
      "|2010-01-26|205.950001|213.710005|202.580004|205.940001|466777500|26.681494|\n",
      "|2010-01-27|206.849995|    210.58|199.530001|207.880005|430642100| 26.93284|\n",
      "|2010-01-28|204.930004|205.500004|198.699995|199.289995|293375600|25.819922|\n",
      "|2010-01-29|201.079996|202.199995|190.250002|192.060003|311488100|24.883208|\n",
      "|2010-02-01|192.369997|       196|191.299999|194.729998|187469100|25.229131|\n",
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "|      Date|      Open|      High|     lower|     Close|   Volume|Adj Close|\n",
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "|2010-01-04|213.429998|214.499996|212.380001|214.009998|123432400|27.727039|\n",
      "|2010-01-05|214.599998|215.589994|213.249994|214.379993|150476200|27.774976|\n",
      "|2010-01-06|214.379993|    215.23|210.750004|210.969995|138040000|27.333178|\n",
      "|2010-01-07|    211.75|212.000006|209.050005|    210.58|119282800| 27.28265|\n",
      "|2010-01-08|210.299994|212.000006|209.060005|211.980005|111902700|27.464034|\n",
      "|2010-01-11|212.799997|213.000002|208.450005|210.110003|115557400|27.221758|\n",
      "|2010-01-12|209.189995|209.769995|206.419998|207.720001|148614900| 26.91211|\n",
      "|2010-01-13|207.870005|210.929995|204.099998|210.650002|151473000| 27.29172|\n",
      "|2010-01-14|210.110003|210.459997|209.020004|    209.43|108223500|27.133657|\n",
      "|2010-01-15|210.929995|211.599997|205.869999|    205.93|148516900|26.680198|\n",
      "|2010-01-19|208.330002|215.189999|207.240004|215.039995|182501900|27.860485|\n",
      "|2010-01-20|214.910006|215.549994|209.500002|    211.73|153038200|27.431644|\n",
      "|2010-01-21|212.079994|213.309996|207.210003|208.069996|152038600|26.957455|\n",
      "|2010-01-22|206.780006|207.499996|    197.16|    197.75|220441900|25.620401|\n",
      "|2010-01-25|202.510002|204.699999|200.190002|203.070002|266424900|26.309658|\n",
      "|2010-01-26|205.950001|213.710005|202.580004|205.940001|466777500|26.681494|\n",
      "|2010-01-27|206.849995|    210.58|199.530001|207.880005|430642100| 26.93284|\n",
      "|2010-01-28|204.930004|205.500004|198.699995|199.289995|293375600|25.819922|\n",
      "|2010-01-29|201.079996|202.199995|190.250002|192.060003|311488100|24.883208|\n",
      "|2010-02-01|192.369997|       196|191.299999|194.729998|187469100|25.229131|\n",
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# task12\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('myapli').getOrCreate()\n",
    "df=spark.read.csv('Tasks - Apple stock.csv',header=True)\n",
    "df.show()\n",
    "res=df.withColumnRenamed('Low','lower')\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "33ef1c95-e31e-4102-b3a3-c7fe0f6283d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "|      Date|      Open|      High|       Low|     Close|   Volume|Adj Close|\n",
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "|2012-02-13|499.529991|503.830009|497.089989|502.600021|129304000|65.116633|\n",
      "|2012-02-14|504.659988| 509.56002|502.000008|509.459991|115099600|66.005408|\n",
      "|2012-02-15|514.259995|526.290016|496.889984|497.669975|376530000|64.477899|\n",
      "|2012-02-16|491.500008|504.890007| 486.62999|502.209999|236138000|65.066102|\n",
      "|2012-02-17|503.109993| 507.77002|500.299995| 502.12001|133951300|65.054443|\n",
      "|2012-02-21|506.880013|514.850021|504.120003|514.850021|151398800|66.703738|\n",
      "|2012-02-22|513.079994|515.489983|509.070023|513.039993|120825600|66.469231|\n",
      "|2012-02-23|515.079987|517.830009|509.499992|516.389977|142006900|66.903253|\n",
      "|2012-02-24|519.669998|522.899979|518.640015|522.409981|103768000|67.683203|\n",
      "|2012-02-27|521.309982|     528.5|516.280014|525.760017|136895500|68.117232|\n",
      "|2012-02-28|527.960014|535.410011|525.850006|535.410011|150096800|69.367481|\n",
      "|2012-02-29|541.560005|547.610023|535.700005|542.440025|238002800|70.278286|\n",
      "|2012-03-01|548.169983|548.209984|538.769981|544.469978|170817500|70.541286|\n",
      "|2012-03-02|544.240013|546.800018|542.519974|545.180008|107928100|70.633277|\n",
      "|2012-03-05|545.420013| 547.47998|526.000023|533.160027|202281100|69.075974|\n",
      "|2012-03-06|523.659996|533.690025|516.219986|530.259987|202559700|68.700246|\n",
      "|2012-03-07|536.800003|537.779999|523.299988| 530.69001|199630200|68.755959|\n",
      "|2012-03-08|534.689995|542.989998|532.120003|541.989975|129114300|70.219978|\n",
      "|2012-03-09|544.209999|547.740013|543.110001|545.170021|104729800|70.631983|\n",
      "|2012-03-12|548.979988|551.999977|547.000023|551.999977|101820600|71.516869|\n",
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# task13\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('myapli').getOrCreate()\n",
    "df=spark.read.csv('Tasks - Apple stock.csv',header=True)\n",
    "# res=df.filter(df['High']>500)\n",
    "res=df.where(df['High']>500)\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d08cc911-4c84-4293-962f-3da27fb99422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "|      Date|      Open|      High|       Low|     Close|   Volume|Adj Close|\n",
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "|2012-02-13|499.529991|503.830009|497.089989|502.600021|129304000|65.116633|\n",
      "|2012-02-16|491.500008|504.890007| 486.62999|502.209999|236138000|65.066102|\n",
      "|2013-01-16|494.639999|509.440018|492.499977|506.089981|172701200|66.151072|\n",
      "|2013-01-18|498.519981|502.219986|496.399986|500.000015|118230700|65.355052|\n",
      "|2013-08-14| 497.88002|504.249992|493.400024|498.500008|189093100|66.408129|\n",
      "|2013-08-15|496.420013|502.400017|489.079979|497.909981|122573500|66.329528|\n",
      "|2013-08-27|498.000023|502.509979|486.299995|488.589981|106047200|65.087956|\n",
      "|2013-09-04|499.560005|502.240013|496.279984|498.690025| 86258200|66.433443|\n",
      "|2013-10-15|497.510025|502.000008| 495.52002|498.679985| 80018400|66.432105|\n",
      "|2013-10-17|499.979988|504.779991|499.680008|504.499985| 63398300|67.207422|\n",
      "|2014-01-31|495.179985|501.529984|493.549988|500.599976|116199300|67.077227|\n",
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# task14\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "spark=SparkSession.builder.appName('myapli').getOrCreate()\n",
    "df=spark.read.csv('Tasks - Apple stock.csv',header=True)\n",
    "# res=df.filter((col('High')>500) & (col('Open')<500 ))\n",
    "# res=df.filter((df.High>500)&(df.Open<500))\n",
    "\n",
    "res=df.filter('High>500 AND Open<500')\n",
    "res.show()\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3fbf56d7-0191-4ea4-9ed9-b79b21569be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "|      Date|      Open|      High|       Low|     Close|   Volume|Adj Close|\n",
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "|2012-02-13|499.529991|503.830009|497.089989|502.600021|129304000|65.116633|\n",
      "|2012-02-16|491.500008|504.890007| 486.62999|502.209999|236138000|65.066102|\n",
      "|2013-01-16|494.639999|509.440018|492.499977|506.089981|172701200|66.151072|\n",
      "|2013-01-18|498.519981|502.219986|496.399986|500.000015|118230700|65.355052|\n",
      "|2013-08-14| 497.88002|504.249992|493.400024|498.500008|189093100|66.408129|\n",
      "|2013-08-15|496.420013|502.400017|489.079979|497.909981|122573500|66.329528|\n",
      "|2013-08-27|498.000023|502.509979|486.299995|488.589981|106047200|65.087956|\n",
      "|2013-09-04|499.560005|502.240013|496.279984|498.690025| 86258200|66.433443|\n",
      "|2013-10-15|497.510025|502.000008| 495.52002|498.679985| 80018400|66.432105|\n",
      "|2013-10-17|499.979988|504.779991|499.680008|504.499985| 63398300|67.207422|\n",
      "|2014-01-31|495.179985|501.529984|493.549988|500.599976|116199300|67.077227|\n",
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# task15\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark=SparkSession.builder.appName('myapli').getOrCreate()\n",
    "df=spark.read.csv('Tasks - Apple stock.csv',header=True)\n",
    "res=df.where((df.High>500)&(df.Open<500))\n",
    "# res=df.where((col('High')>500) & (col('Open')<500 ))\n",
    "\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "598d6855-97af-4d91-b3e0-970d29a2306a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "|      Date|      Open|      High|       Low|     Close|   Volume|Adj Close|\n",
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "|2012-02-13|499.529991|503.830009|497.089989|502.600021|129304000|65.116633|\n",
      "|2012-02-14|504.659988| 509.56002|502.000008|509.459991|115099600|66.005408|\n",
      "|2012-02-15|514.259995|526.290016|496.889984|497.669975|376530000|64.477899|\n",
      "|2012-02-16|491.500008|504.890007| 486.62999|502.209999|236138000|65.066102|\n",
      "|2012-02-17|503.109993| 507.77002|500.299995| 502.12001|133951300|65.054443|\n",
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#task16\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('myapli').getOrCreate()\n",
    "df=spark.read.csv('Tasks - Apple stock.csv',header=True)\n",
    "# res=df.filter(df.High>500).limit(5)\n",
    "res=df.where(df['High']>500).limit(5)\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "efaaabff-329c-49f4-a286-1cf2d1a1b181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "|      Date|      Open|      High|       Low|     Close|   Volume|Adj Close|\n",
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "|2012-02-13|499.529991|503.830009|497.089989|502.600021|129304000|65.116633|\n",
      "|2012-02-16|491.500008|504.890007| 486.62999|502.209999|236138000|65.066102|\n",
      "|2013-01-16|494.639999|509.440018|492.499977|506.089981|172701200|66.151072|\n",
      "|2013-01-18|498.519981|502.219986|496.399986|500.000015|118230700|65.355052|\n",
      "|2013-08-14| 497.88002|504.249992|493.400024|498.500008|189093100|66.408129|\n",
      "|2013-08-15|496.420013|502.400017|489.079979|497.909981|122573500|66.329528|\n",
      "|2013-08-27|498.000023|502.509979|486.299995|488.589981|106047200|65.087956|\n",
      "|2013-09-04|499.560005|502.240013|496.279984|498.690025| 86258200|66.433443|\n",
      "|2013-10-15|497.510025|502.000008| 495.52002|498.679985| 80018400|66.432105|\n",
      "|2013-10-17|499.979988|504.779991|499.680008|504.499985| 63398300|67.207422|\n",
      "|2014-01-31|495.179985|501.529984|493.549988|500.599976|116199300|67.077227|\n",
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#task17\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('myapli').getOrCreate()\n",
    "df=spark.read.csv('Tasks - Apple stock.csv',header=True)\n",
    "res=df.filter('High>500 AND Open<500')\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "08446e37-830e-443d-8eeb-f641da21bf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+------+------+---------+---------+\n",
      "|      Date|      Open|      High|   Low| Close|   Volume|Adj Close|\n",
      "+----------+----------+----------+------+------+---------+---------+\n",
      "|2010-01-22|206.780006|207.499996|197.16|197.75|220441900|25.620401|\n",
      "+----------+----------+----------+------+------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# task18\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('myapli').getOrCreate()\n",
    "df=spark.read.csv('Tasks - Apple stock.csv',header=True)\n",
    "res=df.filter(df.Low==197.16)\n",
    "res.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ebb07905-f543-491f-a67e-96b2da955bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "|      Date|      Open|      High|       Low|     Close|   Volume|Adj Close|\n",
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "|2010-01-04|213.429998|214.499996|212.380001|214.009998|123432400|27.727039|\n",
      "|2010-01-05|214.599998|215.589994|213.249994|214.379993|150476200|27.774976|\n",
      "|2010-01-06|214.379993|    215.23|210.750004|210.969995|138040000|27.333178|\n",
      "|2010-01-07|    211.75|212.000006|209.050005|    210.58|119282800| 27.28265|\n",
      "|2010-01-08|210.299994|212.000006|209.060005|211.980005|111902700|27.464034|\n",
      "|2010-01-11|212.799997|213.000002|208.450005|210.110003|115557400|27.221758|\n",
      "|2010-01-12|209.189995|209.769995|206.419998|207.720001|148614900| 26.91211|\n",
      "|2010-01-13|207.870005|210.929995|204.099998|210.650002|151473000| 27.29172|\n",
      "|2010-01-14|210.110003|210.459997|209.020004|    209.43|108223500|27.133657|\n",
      "|2010-01-15|210.929995|211.599997|205.869999|    205.93|148516900|26.680198|\n",
      "|2010-01-19|208.330002|215.189999|207.240004|215.039995|182501900|27.860485|\n",
      "|2010-01-20|214.910006|215.549994|209.500002|    211.73|153038200|27.431644|\n",
      "|2010-01-21|212.079994|213.309996|207.210003|208.069996|152038600|26.957455|\n",
      "|2010-01-25|202.510002|204.699999|200.190002|203.070002|266424900|26.309658|\n",
      "|2010-01-26|205.950001|213.710005|202.580004|205.940001|466777500|26.681494|\n",
      "|2010-01-27|206.849995|    210.58|199.530001|207.880005|430642100| 26.93284|\n",
      "|2010-02-16|201.940002|203.690002|201.520006|203.399996|135934400|26.352412|\n",
      "|2010-02-17|204.190001|204.310003|200.860004|202.550003|109099200|26.242287|\n",
      "|2010-02-18|201.629995|203.889994|200.920006|202.929998|105706300|26.291519|\n",
      "|2010-02-19|201.860001|203.200005|201.109997|201.669996|103867400|26.128274|\n",
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# task19\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('myapli').getOrCreate()\n",
    "df=spark.read.csv('Tasks - Apple stock.csv',header=True)\n",
    "res=df.filter((df.Open>200) & ~(df.Close<200))\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fac688fa-9f4d-49d1-a3ea-4a268154a00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Date='2010-01-22', Open='206.780006', High='207.499996', Low='197.16', Close='197.75', Volume='220441900', Adj Close='25.620401')\n"
     ]
    }
   ],
   "source": [
    "# task20\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('myapli').getOrCreate()\n",
    "df=spark.read.csv('Tasks - Apple stock.csv',header=True)\n",
    "res=df.filter(df.Low==197.16).collect()\n",
    "for i in res:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ee00f27f-1528-4a3f-a138-60e5b09d0bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+------+------+---------+---------+\n",
      "|      Date|      Open|      High|   Low| Close|   Volume|Adj Close|\n",
      "+----------+----------+----------+------+------+---------+---------+\n",
      "|2010-01-22|206.780006|207.499996|197.16|197.75|220441900|25.620401|\n",
      "+----------+----------+----------+------+------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('myapli').getOrCreate()\n",
    "df=spark.read.csv('Tasks - Apple stock.csv',header=True)\n",
    "res=df.filter(df.Low==197.16)\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7f4d1d5e-1785-4f16-8d6e-1f163cba293b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+---------+----------+---------+---------+\n",
      "|      Date|      Open|      High|      Low|     Close|   Volume|Adj Close|\n",
      "+----------+----------+----------+---------+----------+---------+---------+\n",
      "|2016-07-19| 99.559998|       100|99.339996| 99.870003| 23779900|98.397902|\n",
      "|2014-09-04| 98.849998|100.089996|97.790001| 98.120003| 85718000|93.563292|\n",
      "|2014-10-07|     99.43|100.120003|98.730003|     98.75| 42094200|94.164032|\n",
      "|2016-01-07|     98.68|100.129997|    96.43| 96.449997| 81094400|93.943473|\n",
      "|2016-07-18| 98.699997|100.129997|98.599998| 99.830002| 36493900|98.358491|\n",
      "|2014-10-03| 99.440002|100.209999|99.040001| 99.620003| 43469600|94.993631|\n",
      "|2014-10-02| 99.269997|100.220001|98.040001| 99.900002| 47757800|95.260627|\n",
      "|2016-05-31| 99.599998|100.400002|    98.82| 99.860001| 42307200|98.388047|\n",
      "|2014-09-29| 98.650002|100.440002|98.629997|100.110001| 49766300|95.460874|\n",
      "|2016-07-20|       100|100.459999|99.739998| 99.959999| 26276000|98.486572|\n",
      "|2016-05-27| 99.440002|100.470001|    99.25|100.349998| 36229500|98.870823|\n",
      "|2016-01-14| 97.959999|100.480003|95.739998| 99.519997| 63170100| 96.93369|\n",
      "|2014-10-14|100.389999|100.519997|    98.57|     98.75| 63688600|94.164032|\n",
      "|2014-10-06| 99.949997|100.650002|99.419998| 99.620003| 37051200|94.993631|\n",
      "|2014-08-19| 99.410004|    100.68|    99.32|100.529999| 69399000|95.861367|\n",
      "|2016-01-12|100.550003|100.690002|98.839996| 99.959999| 49154200|97.362258|\n",
      "|2014-10-01|100.589996|100.690002|98.699997|     99.18| 51491300|94.574063|\n",
      "|2014-09-25|100.510002|100.709999|97.720001| 97.870003|100092000|93.324902|\n",
      "|2016-05-26|     99.68|100.730003|98.639999|100.410004| 56331200|98.929943|\n",
      "|2014-09-26| 98.529999|    100.75|98.400002|    100.75| 62370500|96.071151|\n",
      "+----------+----------+----------+---------+----------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# task21\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('myapli').getOrCreate()\n",
    "df=spark.read.csv('Tasks - Apple stock.csv',header=True)\n",
    "# res=df.sort('High',ascending=False)\n",
    "res=df.sort('High')\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "694124ad-94c2-4e56-8554-2d9e2b80a9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+---------+----------+---------+---------+\n",
      "|      Date|      Open|      High|      Low|     Close|   Volume|Adj Close|\n",
      "+----------+----------+----------+---------+----------+---------+---------+\n",
      "|2016-07-19| 99.559998|       100|99.339996| 99.870003| 23779900|98.397902|\n",
      "|2014-09-04| 98.849998|100.089996|97.790001| 98.120003| 85718000|93.563292|\n",
      "|2014-10-07|     99.43|100.120003|98.730003|     98.75| 42094200|94.164032|\n",
      "|2016-01-07|     98.68|100.129997|    96.43| 96.449997| 81094400|93.943473|\n",
      "|2016-07-18| 98.699997|100.129997|98.599998| 99.830002| 36493900|98.358491|\n",
      "|2014-10-03| 99.440002|100.209999|99.040001| 99.620003| 43469600|94.993631|\n",
      "|2014-10-02| 99.269997|100.220001|98.040001| 99.900002| 47757800|95.260627|\n",
      "|2016-05-31| 99.599998|100.400002|    98.82| 99.860001| 42307200|98.388047|\n",
      "|2014-09-29| 98.650002|100.440002|98.629997|100.110001| 49766300|95.460874|\n",
      "|2016-07-20|       100|100.459999|99.739998| 99.959999| 26276000|98.486572|\n",
      "|2016-05-27| 99.440002|100.470001|    99.25|100.349998| 36229500|98.870823|\n",
      "|2016-01-14| 97.959999|100.480003|95.739998| 99.519997| 63170100| 96.93369|\n",
      "|2014-10-14|100.389999|100.519997|    98.57|     98.75| 63688600|94.164032|\n",
      "|2014-10-06| 99.949997|100.650002|99.419998| 99.620003| 37051200|94.993631|\n",
      "|2014-08-19| 99.410004|    100.68|    99.32|100.529999| 69399000|95.861367|\n",
      "|2016-01-12|100.550003|100.690002|98.839996| 99.959999| 49154200|97.362258|\n",
      "|2014-10-01|100.589996|100.690002|98.699997|     99.18| 51491300|94.574063|\n",
      "|2014-09-25|100.510002|100.709999|97.720001| 97.870003|100092000|93.324902|\n",
      "|2016-05-26|     99.68|100.730003|98.639999|100.410004| 56331200|98.929943|\n",
      "|2014-09-26| 98.529999|    100.75|98.400002|    100.75| 62370500|96.071151|\n",
      "+----------+----------+----------+---------+----------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# task22\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('myapli').getOrCreate()\n",
    "df=spark.read.csv('Tasks - Apple stock.csv',header=True)\n",
    "# res=df.sort('High',ascending=False)\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "bc9293b1-a865-4964-bc1f-ab91ca50339f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|Company|Sales|\n",
      "+-------+-----+\n",
      "|   APPL|  130|\n",
      "|   APPL|  250|\n",
      "|   APPL|  350|\n",
      "|   APPL|  750|\n",
      "|     FB|  350|\n",
      "|     FB|  870|\n",
      "|   GOOG|  120|\n",
      "|   GOOG|  200|\n",
      "|   GOOG|  340|\n",
      "|   MSFT|  124|\n",
      "|   MSFT|  243|\n",
      "|   MSFT|  600|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# task23\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import desc\n",
    "spark=SparkSession.builder.appName('myapli').getOrCreate()\n",
    "df=spark.read.csv('Tasks - Sales_info.csv',header=True)\n",
    "res=df.orderBy(['company','Sales'])\n",
    "res.select('Company','Sales').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "36edd0b5-98a9-4fed-a4a8-fd72153058db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+----+\n",
      "|   name|age|     dept| sal|\n",
      "+-------+---+---------+----+\n",
      "|  Alice| 30|    Sales|5000|\n",
      "|    Bob| 25|Marketing|7000|\n",
      "|Charlie| 35|    Sales|6000|\n",
      "|  David| 28|Marketing|8000|\n",
      "+-------+---+---------+----+\n",
      "\n",
      "+-------+---+---------+----+\n",
      "|   name|age|     dept| sal|\n",
      "+-------+---+---------+----+\n",
      "|  Alice| 30|    Sales|5000|\n",
      "|Charlie| 35|    Sales|6000|\n",
      "|  David| 28|Marketing|8000|\n",
      "+-------+---+---------+----+\n",
      "\n",
      "stock\n",
      "student\n"
     ]
    }
   ],
   "source": [
    "#task24\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import *\n",
    "\n",
    "spark=SparkSession.builder.appName('myapli').getOrCreate()\n",
    "data=[(\"Alice\", 30, \"Sales\", 5000),\n",
    "        (\"Bob\", 25, \"Marketing\", 7000),\n",
    "        (\"Charlie\", 35, \"Sales\", 6000),\n",
    "        (\"David\", 28, \"Marketing\", 8000)]\n",
    "col=['name','age','dept','sal']\n",
    "df=spark.createDataFrame(data,col)\n",
    "df.show()\n",
    "\n",
    "temp=df.createOrReplaceTempView('student')\n",
    "res=spark.sql('SELECT * from stock where age>25')\n",
    "res.show()\n",
    "spark.catalog.dropTempView('stock1')\n",
    "res1=spark.catalog.listTables()\n",
    "for i in res1:\n",
    "    print(i.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ce890a4-fa90-4d3c-8e83-419b4f32c97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+----+\n",
      "|   name|age|     dept| sal|\n",
      "+-------+---+---------+----+\n",
      "|  Alice| 30|    Sales|5000|\n",
      "|    Bob| 25|Marketing|7000|\n",
      "|Charlie| 35|    Sales|6000|\n",
      "|  David| 28|Marketing|8000|\n",
      "+-------+---+---------+----+\n",
      "\n",
      "\n",
      "+-------+---+---------+----+-----+\n",
      "|   name|age|     dept| sal|bonus|\n",
      "+-------+---+---------+----+-----+\n",
      "|  Alice| 30|    Sales|5000|500.0|\n",
      "|    Bob| 25|Marketing|7000|700.0|\n",
      "|Charlie| 35|    Sales|6000|600.0|\n",
      "|  David| 28|Marketing|8000|800.0|\n",
      "+-------+---+---------+----+-----+\n",
      "\n",
      "\n",
      "+-------+---+---------+----+\n",
      "|   name|age|     dept| sal|\n",
      "+-------+---+---------+----+\n",
      "|  David| 28|Marketing|8000|\n",
      "|    Bob| 25|Marketing|7000|\n",
      "|Charlie| 35|    Sales|6000|\n",
      "|  Alice| 30|    Sales|5000|\n",
      "+-------+---+---------+----+\n",
      "\n",
      "\n",
      "+---------+----------+--------+\n",
      "|     dept|avg(bonus)|max(sal)|\n",
      "+---------+----------+--------+\n",
      "|    Sales|     550.0|    6000|\n",
      "|Marketing|     750.0|    8000|\n",
      "+---------+----------+--------+\n",
      "\n",
      "+-------+---------+\n",
      "|   name|     dept|\n",
      "+-------+---------+\n",
      "|  Alice|    Sales|\n",
      "|    Bob|Marketing|\n",
      "|Charlie|    Sales|\n",
      "|  David|Marketing|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TASK25\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('myapli').getOrCreate()\n",
    "data=[(\"Alice\", 30, \"Sales\", 5000),\n",
    "        (\"Bob\", 25, \"Marketing\", 7000),\n",
    "        (\"Charlie\", 35, \"Sales\", 6000),\n",
    "        (\"David\", 28, \"Marketing\", 8000)]\n",
    "col=['name','age','dept','sal']\n",
    "df=spark.createDataFrame(data,col)\n",
    "df.show()\n",
    "print()\n",
    "res=df.withColumn('bonus',df.sal*0.1)\n",
    "res.show()\n",
    "print()\n",
    "df=df.orderBy(df.sal,ascending=False)\n",
    "df.show()\n",
    "print()\n",
    "res2=res.groupBy('dept').agg({'sal':'max','bonus':'avg'})\n",
    "res2.show()\n",
    "res3=res.select(['name','dept'])\n",
    "res3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "688dd7f2-0428-4144-bc8c-b5d1e40604fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+--------------------+--------------------+-----------------+--------------------+-------------+-----------------+--------------------+----+--------------+--------------------+-------------+\n",
      "|_corrupt_record|                 _id|  amazon_product_url|              author| bestsellers_date|         description|        price|   published_date|           publisher|rank|rank_last_week|               title|weeks_on_list|\n",
      "+---------------+--------------------+--------------------+--------------------+-----------------+--------------------+-------------+-----------------+--------------------+----+--------------+--------------------+-------------+\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|       Dean R Koontz|{{1211587200000}}|Odd Thomas, who c...|   {NULL, 27}|{{1212883200000}}|              Bantam| {1}|           {0}|           ODD HOURS|          {1}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|     Stephenie Meyer|{{1211587200000}}|Aliens have taken...|{25.99, NULL}|{{1212883200000}}|       Little, Brown| {2}|           {1}|            THE HOST|          {3}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|        Emily Giffin|{{1211587200000}}|A woman's happy m...|{24.95, NULL}|{{1212883200000}}|        St. Martin's| {3}|           {2}|LOVE THE ONE YOU'...|          {2}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|   Patricia Cornwell|{{1211587200000}}|A Massachusetts s...|{22.95, NULL}|{{1212883200000}}|              Putnam| {4}|           {0}|           THE FRONT|          {1}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|     Chuck Palahniuk|{{1211587200000}}|An aging porn que...|{24.95, NULL}|{{1212883200000}}|           Doubleday| {5}|           {0}|               SNUFF|          {1}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|James Patterson a...|{{1211587200000}}|A woman finds an ...|{24.99, NULL}|{{1212883200000}}|       Little, Brown| {6}|           {3}|SUNDAYS AT TIFFANYS|          {4}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|       John Sandford|{{1211587200000}}|The Minneapolis d...|{26.95, NULL}|{{1212883200000}}|              Putnam| {7}|           {4}|        PHANTOM PREY|          {3}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|       Jimmy Buffett|{{1211587200000}}|A Southern family...|{21.99, NULL}|{{1212883200000}}|       Little, Brown| {8}|           {6}|          SWINE NOT?|          {2}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|    Elizabeth George|{{1211587200000}}|In Cornwall, tryi...|{27.95, NULL}|{{1212883200000}}|              Harper| {9}|           {8}|     CARELESS IN RED|          {3}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|      David Baldacci|{{1211587200000}}|An intelligence a...|{26.99, NULL}|{{1212883200000}}|       Grand Central|{10}|           {7}|     THE WHOLE TRUTH|          {5}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|        Troy Denning|{{1211587200000}}|The New Jedi orde...|   {NULL, 27}|{{1212883200000}}|  Del Rey/Ballantine|{11}|           {5}|          INVINCIBLE|          {2}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|          James Frey|{{1211587200000}}|A novel, set in L...|{26.95, NULL}|{{1212883200000}}|              Harper|{12}|           {9}|BRIGHT SHINY MORNING|          {2}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|         Garth Stein|{{1211587200000}}|A Lab-terrier mix...|{23.95, NULL}|{{1212883200000}}|              Harper|{13}|           {0}|THE ART OF RACING...|          {1}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|     Debbie Macomber|{{1211587200000}}|A widow who owns ...|{24.95, NULL}|{{1212883200000}}|                Mira|{14}|          {10}|       TWENTY WISHES|          {4}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|         Jeff Shaara|{{1211587200000}}|A novel about the...|   {NULL, 28}|{{1212883200000}}|          Ballantine|{15}|          {11}|      THE STEEL WAVE|          {2}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|    Phillip Margolin|{{1211587200000}}|                    |    {NULL, 0}|{{1212883200000}}|HarperCollins Pub...|{16}|           {0}| EXECUTIVE PRIVILEGE|          {0}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|       Jhumpa Lahiri|{{1211587200000}}|Stories of the an...|    {NULL, 0}|{{1212883200000}}|               Knopf|{17}|           {0}|  UNACCUSTOMED EARTH|          {0}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|      Joseph O'Neill|{{1211587200000}}|A Dutchman desert...|    {NULL, 0}|{{1212883200000}}|Knopf Publishing ...|{18}|           {0}|          NETHERLAND|          {0}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|        John Grisham|{{1211587200000}}|Political and leg...|    {NULL, 0}|{{1212883200000}}|Doubleday Publishing|{19}|           {0}|          THE APPEAL|          {0}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|       James Rollins|{{1211587200000}}|                    |    {NULL, 0}|{{1212883200000}}|Random House Publ...|{20}|           {0}|INDIANA JONES AND...|          {0}|\n",
      "+---------------+--------------------+--------------------+--------------------+-----------------+--------------------+-------------+-----------------+--------------------+----+--------------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sql task1\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('app').getOrCreate()\n",
    "df=spark.read.json('Dataframe_sql.json')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "6ffb7958-8486-4a9f-8ed3-1d30c61afb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+--------------------+--------------------+-----------------+--------------------+-------------+-----------------+--------------------+----+--------------+--------------------+-------------+\n",
      "|_corrupt_record|                 _id|  amazon_product_url|              author| bestsellers_date|         description|        price|   published_date|           publisher|rank|rank_last_week|               title|weeks_on_list|\n",
      "+---------------+--------------------+--------------------+--------------------+-----------------+--------------------+-------------+-----------------+--------------------+----+--------------+--------------------+-------------+\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|       Dean R Koontz|{{1211587200000}}|Odd Thomas, who c...|   {NULL, 27}|{{1212883200000}}|              Bantam| {1}|           {0}|           ODD HOURS|          {1}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|     Stephenie Meyer|{{1211587200000}}|Aliens have taken...|{25.99, NULL}|{{1212883200000}}|       Little, Brown| {2}|           {1}|            THE HOST|          {3}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|        Emily Giffin|{{1211587200000}}|A woman's happy m...|{24.95, NULL}|{{1212883200000}}|        St. Martin's| {3}|           {2}|LOVE THE ONE YOU'...|          {2}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|   Patricia Cornwell|{{1211587200000}}|A Massachusetts s...|{22.95, NULL}|{{1212883200000}}|              Putnam| {4}|           {0}|           THE FRONT|          {1}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|     Chuck Palahniuk|{{1211587200000}}|An aging porn que...|{24.95, NULL}|{{1212883200000}}|           Doubleday| {5}|           {0}|               SNUFF|          {1}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|James Patterson a...|{{1211587200000}}|A woman finds an ...|{24.99, NULL}|{{1212883200000}}|       Little, Brown| {6}|           {3}|SUNDAYS AT TIFFANYS|          {4}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|       John Sandford|{{1211587200000}}|The Minneapolis d...|{26.95, NULL}|{{1212883200000}}|              Putnam| {7}|           {4}|        PHANTOM PREY|          {3}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|       Jimmy Buffett|{{1211587200000}}|A Southern family...|{21.99, NULL}|{{1212883200000}}|       Little, Brown| {8}|           {6}|          SWINE NOT?|          {2}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|    Elizabeth George|{{1211587200000}}|In Cornwall, tryi...|{27.95, NULL}|{{1212883200000}}|              Harper| {9}|           {8}|     CARELESS IN RED|          {3}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|      David Baldacci|{{1211587200000}}|An intelligence a...|{26.99, NULL}|{{1212883200000}}|       Grand Central|{10}|           {7}|     THE WHOLE TRUTH|          {5}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|        Troy Denning|{{1211587200000}}|The New Jedi orde...|   {NULL, 27}|{{1212883200000}}|  Del Rey/Ballantine|{11}|           {5}|          INVINCIBLE|          {2}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|          James Frey|{{1211587200000}}|A novel, set in L...|{26.95, NULL}|{{1212883200000}}|              Harper|{12}|           {9}|BRIGHT SHINY MORNING|          {2}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|         Garth Stein|{{1211587200000}}|A Lab-terrier mix...|{23.95, NULL}|{{1212883200000}}|              Harper|{13}|           {0}|THE ART OF RACING...|          {1}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|     Debbie Macomber|{{1211587200000}}|A widow who owns ...|{24.95, NULL}|{{1212883200000}}|                Mira|{14}|          {10}|       TWENTY WISHES|          {4}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|         Jeff Shaara|{{1211587200000}}|A novel about the...|   {NULL, 28}|{{1212883200000}}|          Ballantine|{15}|          {11}|      THE STEEL WAVE|          {2}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|    Phillip Margolin|{{1211587200000}}|                    |    {NULL, 0}|{{1212883200000}}|HarperCollins Pub...|{16}|           {0}| EXECUTIVE PRIVILEGE|          {0}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|       Jhumpa Lahiri|{{1211587200000}}|Stories of the an...|    {NULL, 0}|{{1212883200000}}|               Knopf|{17}|           {0}|  UNACCUSTOMED EARTH|          {0}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|      Joseph O'Neill|{{1211587200000}}|A Dutchman desert...|    {NULL, 0}|{{1212883200000}}|Knopf Publishing ...|{18}|           {0}|          NETHERLAND|          {0}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|        John Grisham|{{1211587200000}}|Political and leg...|    {NULL, 0}|{{1212883200000}}|Doubleday Publishing|{19}|           {0}|          THE APPEAL|          {0}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|       James Rollins|{{1211587200000}}|                    |    {NULL, 0}|{{1212883200000}}|Random House Publ...|{20}|           {0}|INDIANA JONES AND...|          {0}|\n",
      "+---------------+--------------------+--------------------+--------------------+-----------------+--------------------+-------------+-----------------+--------------------+----+--------------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sql task2\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('app').getOrCreate()\n",
    "df=spark.read.json('Dataframe_sql.json')\n",
    "df.createOrReplaceTempView('stock')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b9333a8-475c-436f-9477-19cfb1a18ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+--------------------+--------------------+-----------------+--------------------+-------------+-----------------+--------------------+----+--------------+--------------------+-------------+\n",
      "|_corrupt_record|                 _id|  amazon_product_url|              author| bestsellers_date|         description|        price|   published_date|           publisher|rank|rank_last_week|               title|weeks_on_list|\n",
      "+---------------+--------------------+--------------------+--------------------+-----------------+--------------------+-------------+-----------------+--------------------+----+--------------+--------------------+-------------+\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|       Dean R Koontz|{{1211587200000}}|Odd Thomas, who c...|   {NULL, 27}|{{1212883200000}}|              Bantam| {1}|           {0}|           ODD HOURS|          {1}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|     Stephenie Meyer|{{1211587200000}}|Aliens have taken...|{25.99, NULL}|{{1212883200000}}|       Little, Brown| {2}|           {1}|            THE HOST|          {3}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|        Emily Giffin|{{1211587200000}}|A woman's happy m...|{24.95, NULL}|{{1212883200000}}|        St. Martin's| {3}|           {2}|LOVE THE ONE YOU'...|          {2}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|   Patricia Cornwell|{{1211587200000}}|A Massachusetts s...|{22.95, NULL}|{{1212883200000}}|              Putnam| {4}|           {0}|           THE FRONT|          {1}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|     Chuck Palahniuk|{{1211587200000}}|An aging porn que...|{24.95, NULL}|{{1212883200000}}|           Doubleday| {5}|           {0}|               SNUFF|          {1}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|James Patterson a...|{{1211587200000}}|A woman finds an ...|{24.99, NULL}|{{1212883200000}}|       Little, Brown| {6}|           {3}|SUNDAYS AT TIFFANYS|          {4}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|       John Sandford|{{1211587200000}}|The Minneapolis d...|{26.95, NULL}|{{1212883200000}}|              Putnam| {7}|           {4}|        PHANTOM PREY|          {3}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|       Jimmy Buffett|{{1211587200000}}|A Southern family...|{21.99, NULL}|{{1212883200000}}|       Little, Brown| {8}|           {6}|          SWINE NOT?|          {2}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|    Elizabeth George|{{1211587200000}}|In Cornwall, tryi...|{27.95, NULL}|{{1212883200000}}|              Harper| {9}|           {8}|     CARELESS IN RED|          {3}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|      David Baldacci|{{1211587200000}}|An intelligence a...|{26.99, NULL}|{{1212883200000}}|       Grand Central|{10}|           {7}|     THE WHOLE TRUTH|          {5}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|        Troy Denning|{{1211587200000}}|The New Jedi orde...|   {NULL, 27}|{{1212883200000}}|  Del Rey/Ballantine|{11}|           {5}|          INVINCIBLE|          {2}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|          James Frey|{{1211587200000}}|A novel, set in L...|{26.95, NULL}|{{1212883200000}}|              Harper|{12}|           {9}|BRIGHT SHINY MORNING|          {2}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|         Garth Stein|{{1211587200000}}|A Lab-terrier mix...|{23.95, NULL}|{{1212883200000}}|              Harper|{13}|           {0}|THE ART OF RACING...|          {1}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|     Debbie Macomber|{{1211587200000}}|A widow who owns ...|{24.95, NULL}|{{1212883200000}}|                Mira|{14}|          {10}|       TWENTY WISHES|          {4}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|         Jeff Shaara|{{1211587200000}}|A novel about the...|   {NULL, 28}|{{1212883200000}}|          Ballantine|{15}|          {11}|      THE STEEL WAVE|          {2}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|    Phillip Margolin|{{1211587200000}}|                    |    {NULL, 0}|{{1212883200000}}|HarperCollins Pub...|{16}|           {0}| EXECUTIVE PRIVILEGE|          {0}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|       Jhumpa Lahiri|{{1211587200000}}|Stories of the an...|    {NULL, 0}|{{1212883200000}}|               Knopf|{17}|           {0}|  UNACCUSTOMED EARTH|          {0}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|      Joseph O'Neill|{{1211587200000}}|A Dutchman desert...|    {NULL, 0}|{{1212883200000}}|Knopf Publishing ...|{18}|           {0}|          NETHERLAND|          {0}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|        John Grisham|{{1211587200000}}|Political and leg...|    {NULL, 0}|{{1212883200000}}|Doubleday Publishing|{19}|           {0}|          THE APPEAL|          {0}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|       James Rollins|{{1211587200000}}|                    |    {NULL, 0}|{{1212883200000}}|Random House Publ...|{20}|           {0}|INDIANA JONES AND...|          {0}|\n",
      "+---------------+--------------------+--------------------+--------------------+-----------------+--------------------+-------------+-----------------+--------------------+----+--------------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "name stock exisit in tmpview\n",
      "['stock']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"AppleStockAnalysis\").getOrCreate()\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "csv_file_path = \"Dataframe_sql.json\"  # Replace with the actual path to your CSV file\n",
    "df = spark.read.json(csv_file_path)\n",
    "\n",
    "# Create a temporary view named \"stock\"\n",
    "df.createOrReplaceTempView(\"stock\")\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "temp_view=spark.catalog.listTables()\n",
    "if 'stock' in [temp.name for temp in temp_view]:\n",
    "    print('name stock exisit in tmpview')\n",
    "else:\n",
    "    print('name stock doenst exisit')\n",
    "res=[i.name for i in temp_view]\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "6b91cbca-766f-4e59-a713-177c18ea755d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "|      Date|      Open|      High|       Low|     Close|   Volume|Adj Close|\n",
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "|2010-01-04|213.429998|214.499996|212.380001|214.009998|123432400|27.727039|\n",
      "|2010-01-05|214.599998|215.589994|213.249994|214.379993|150476200|27.774976|\n",
      "|2010-01-06|214.379993|    215.23|210.750004|210.969995|138040000|27.333178|\n",
      "|2010-01-07|    211.75|212.000006|209.050005|    210.58|119282800| 27.28265|\n",
      "|2010-01-08|210.299994|212.000006|209.060005|211.980005|111902700|27.464034|\n",
      "|2010-01-11|212.799997|213.000002|208.450005|210.110003|115557400|27.221758|\n",
      "|2010-01-12|209.189995|209.769995|206.419998|207.720001|148614900| 26.91211|\n",
      "|2010-01-13|207.870005|210.929995|204.099998|210.650002|151473000| 27.29172|\n",
      "|2010-01-14|210.110003|210.459997|209.020004|    209.43|108223500|27.133657|\n",
      "|2010-01-15|210.929995|211.599997|205.869999|    205.93|148516900|26.680198|\n",
      "|2010-01-19|208.330002|215.189999|207.240004|215.039995|182501900|27.860485|\n",
      "|2010-01-20|214.910006|215.549994|209.500002|    211.73|153038200|27.431644|\n",
      "|2010-01-21|212.079994|213.309996|207.210003|208.069996|152038600|26.957455|\n",
      "|2010-01-26|205.950001|213.710005|202.580004|205.940001|466777500|26.681494|\n",
      "|2010-02-16|201.940002|203.690002|201.520006|203.399996|135934400|26.352412|\n",
      "|2010-02-19|201.860001|203.200005|201.109997|201.669996|103867400|26.128274|\n",
      "|2010-02-26|202.379999|205.169996|202.000004|204.619997|126865200|26.510475|\n",
      "|2010-03-01|205.749996|209.500002|205.450003|208.990004|137523400|27.076651|\n",
      "|2010-03-02|209.929998|210.830006|207.740002|    208.85|141636600|27.058512|\n",
      "|2010-03-03|208.940002|209.869997|207.940006|209.329998| 93013200|27.120701|\n",
      "+----------+----------+----------+----------+----------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sql task3\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('app').getOrCreate()\n",
    "df=spark.read.csv('Tasks - Apple stock.csv',header=True)\n",
    "df.createOrReplaceTempView('stock')\n",
    "res=spark.sql('select * from stock where Low>200')\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5dac0c4d-206d-4f93-9e8f-ed36fffcd0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     400|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sql task4\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('app').getOrCreate()\n",
    "df=spark.read.csv('Tasks - Apple stock.csv',header=True)\n",
    "df.createOrReplaceTempView('stock')\n",
    "res=spark.sql('select count(*) from stock where Close>500')\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1cb0dead-831a-48d5-a9a3-ead821baee7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+--------------------+--------------------+-----------------+--------------------+-------------+-----------------+----------------+----+--------------+--------------------+-------------+\n",
      "|_corrupt_record|                 _id|  amazon_product_url|              author| bestsellers_date|         description|        price|   published_date|       publisher|rank|rank_last_week|               title|weeks_on_list|\n",
      "+---------------+--------------------+--------------------+--------------------+-----------------+--------------------+-------------+-----------------+----------------+----+--------------+--------------------+-------------+\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|        Daniel Silva|{{1217030400000}}|Gabriel Allon, an...|{26.95, NULL}|{{1218326400000}}|          Putnam| {1}|           {0}|  THE SECRET SERVANT|          {1}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|          Jane Green|{{1218240000000}}|A womans life ch...|    {NULL, 0}|{{1219536000000}}|          Viking|{18}|           {0}|     THE BEACH HOUSE|          {0}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|            J D Robb|{{1226102400000}}|Lt. Eve Dallas in...|{25.95, NULL}|{{1227398400000}}|          Putnam| {2}|           {0}|  SALVATION IN DEATH|          {1}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|       Toni Morrison|{{1226707200000}}|In 17th-century A...|{23.95, NULL}|{{1228003200000}}|           Knopf| {5}|           {0}|             A MERCY|          {1}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|     Nicholas Sparks|{{1232150400000}}|A Marine returnin...|    {NULL, 0}|{{1233446400000}}|   Grand Central|{16}|           {0}|       THE LUCKY ONE|          {0}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|           Brad Thor|{{1217030400000}}|Scot Harvath, a H...|   {NULL, 26}|{{1218326400000}}|           Atria| {5}|           {5}|    THE LAST PATRIOT|          {4}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|    David Wroblewski|{{1218844800000}}|A mute takes refu...|{25.95, NULL}|{{1220140800000}}|            Ecco| {7}|           {6}|THE STORY OF EDGA...|         {10}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|    David Wroblewski|{{1219449600000}}|A mute takes refu...|{25.95, NULL}|{{1220745600000}}|            Ecco| {8}|           {7}|THE STORY OF EDGA...|         {11}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|     Debbie Macomber|{{1224288000000}}|A pregnant woman ...|{16.95, NULL}|{{1225584000000}}|            Mira|{14}|          {13}|A CEDAR COVE CHRI...|          {3}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|   Linda Lael Miller|{{1225497600000}}|                    |    {NULL, 0}|{{1226793600000}}|             HQN|{18}|           {0}|A MCKETTRICK CHRI...|          {0}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|Mary Ann Shaffer ...|{{1233964800000}}|A journalist trav...|   {NULL, 22}|{{1235260800000}}|            Dial| {8}|           {9}|THE GUERNSEY LITE...|         {21}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|Clive Cussler wit...|{{1212796800000}}|Juan Cabrillo and...|{26.95, NULL}|{{1214092800000}}|          Putnam| {3}|           {0}|         PLAGUE SHIP|          {1}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|       Dennis Lehane|{{1223078400000}}|A policeman, a fu...|{27.95, NULL}|{{1224374400000}}|          Morrow| {9}|           {3}|       THE GIVEN DAY|          {2}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|        Stephen King|{{1226707200000}}|Short stories ble...|   {NULL, 28}|{{1228003200000}}|        Scribner| {2}|           {0}|   JUST AFTER SUNSET|          {1}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|   Patricia Cornwell|{{1229731200000}}|The forensic path...|{27.95, NULL}|{{1231027200000}}|          Putnam| {2}|           {1}|           SCARPETTA|          {3}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|      Faye Kellerman|{{1218844800000}}|Decker and Lazaru...|{25.95, NULL}|{{1220140800000}}|          Morrow| {8}|           {0}| THE MERCEDES COFFIN|          {1}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|     James Patterson|{{1230940800000}}|Alex Cross chases...|{27.99, NULL}|{{1232236800000}}|   Little, Brown| {4}|           {2}|       CROSS COUNTRY|          {7}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|       Stieg Larsson|{{1231545600000}}|A hacker and a jo...|    {NULL, 0}|{{1232841600000}}|           Knopf|{18}|           {0}|THE GIRL WITH THE...|          {0}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|   Lauren Weisberger|{{1214611200000}}|Three glamorous f...|{25.95, NULL}|{{1215907200000}}|Simon & Schuster| {8}|           {4}|CHASING HARRY WIN...|          {5}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|     Janet Evanovich|{{1215820800000}}|Stephanie Plum an...|{27.95, NULL}|{{1217116800000}}|    St. Martins| {2}|           {2}|   FEARLESS FOURTEEN|          {4}|\n",
      "+---------------+--------------------+--------------------+--------------------+-----------------+--------------------+-------------+-----------------+----------------+----+--------------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sql task5\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('app').getOrCreate()\n",
    "df=spark.read.json('Dataframe_sql.json')\n",
    "res=df.dropDuplicates()\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "eaa96e62-e2fe-4fc3-84e8-803fb6a5a4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+--------------------+--------------------+-----------------+--------------------+-------------+-----------------+--------------------+----+--------------+--------------------+-------------+\n",
      "|_corrupt_record|                 _id|  amazon_product_url|              author| bestsellers_date|         description|        price|   published_date|           publisher|rank|rank_last_week|               title|weeks_on_list|\n",
      "+---------------+--------------------+--------------------+--------------------+-----------------+--------------------+-------------+-----------------+--------------------+----+--------------+--------------------+-------------+\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|       Dean R Koontz|{{1211587200000}}|Odd Thomas, who c...|   {NULL, 27}|{{1212883200000}}|              Bantam| {1}|           {0}|           ODD HOURS|          {1}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|     Stephenie Meyer|{{1211587200000}}|Aliens have taken...|{25.99, NULL}|{{1212883200000}}|       Little, Brown| {2}|           {1}|            THE HOST|          {3}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|        Emily Giffin|{{1211587200000}}|A woman's happy m...|{24.95, NULL}|{{1212883200000}}|        St. Martin's| {3}|           {2}|LOVE THE ONE YOU'...|          {2}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|   Patricia Cornwell|{{1211587200000}}|A Massachusetts s...|{22.95, NULL}|{{1212883200000}}|              Putnam| {4}|           {0}|           THE FRONT|          {1}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|     Chuck Palahniuk|{{1211587200000}}|An aging porn que...|{24.95, NULL}|{{1212883200000}}|           Doubleday| {5}|           {0}|               SNUFF|          {1}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|James Patterson a...|{{1211587200000}}|A woman finds an ...|{24.99, NULL}|{{1212883200000}}|       Little, Brown| {6}|           {3}|SUNDAYS AT TIFFANYS|          {4}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|       John Sandford|{{1211587200000}}|The Minneapolis d...|{26.95, NULL}|{{1212883200000}}|              Putnam| {7}|           {4}|        PHANTOM PREY|          {3}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|       Jimmy Buffett|{{1211587200000}}|A Southern family...|{21.99, NULL}|{{1212883200000}}|       Little, Brown| {8}|           {6}|          SWINE NOT?|          {2}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|    Elizabeth George|{{1211587200000}}|In Cornwall, tryi...|{27.95, NULL}|{{1212883200000}}|              Harper| {9}|           {8}|     CARELESS IN RED|          {3}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|      David Baldacci|{{1211587200000}}|An intelligence a...|{26.99, NULL}|{{1212883200000}}|       Grand Central|{10}|           {7}|     THE WHOLE TRUTH|          {5}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|        Troy Denning|{{1211587200000}}|The New Jedi orde...|   {NULL, 27}|{{1212883200000}}|  Del Rey/Ballantine|{11}|           {5}|          INVINCIBLE|          {2}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|          James Frey|{{1211587200000}}|A novel, set in L...|{26.95, NULL}|{{1212883200000}}|              Harper|{12}|           {9}|BRIGHT SHINY MORNING|          {2}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|         Garth Stein|{{1211587200000}}|A Lab-terrier mix...|{23.95, NULL}|{{1212883200000}}|              Harper|{13}|           {0}|THE ART OF RACING...|          {1}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|     Debbie Macomber|{{1211587200000}}|A widow who owns ...|{24.95, NULL}|{{1212883200000}}|                Mira|{14}|          {10}|       TWENTY WISHES|          {4}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|         Jeff Shaara|{{1211587200000}}|A novel about the...|   {NULL, 28}|{{1212883200000}}|          Ballantine|{15}|          {11}|      THE STEEL WAVE|          {2}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|    Phillip Margolin|{{1211587200000}}|                    |    {NULL, 0}|{{1212883200000}}|HarperCollins Pub...|{16}|           {0}| EXECUTIVE PRIVILEGE|          {0}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|       Jhumpa Lahiri|{{1211587200000}}|Stories of the an...|    {NULL, 0}|{{1212883200000}}|               Knopf|{17}|           {0}|  UNACCUSTOMED EARTH|          {0}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|      Joseph O'Neill|{{1211587200000}}|A Dutchman desert...|    {NULL, 0}|{{1212883200000}}|Knopf Publishing ...|{18}|           {0}|          NETHERLAND|          {0}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|        John Grisham|{{1211587200000}}|Political and leg...|    {NULL, 0}|{{1212883200000}}|Doubleday Publishing|{19}|           {0}|          THE APPEAL|          {0}|\n",
      "|           NULL|{5b4aa4ead3089013...|http://www.amazon...|       James Rollins|{{1211587200000}}|                    |    {NULL, 0}|{{1212883200000}}|Random House Publ...|{20}|           {0}|INDIANA JONES AND...|          {0}|\n",
      "+---------------+--------------------+--------------------+--------------------+-----------------+--------------------+-------------+-----------------+--------------------+----+--------------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "# sql task6\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('apli').getOrCreate()\n",
    "df=spark.read.json('Dataframe_sql.json')\n",
    "df.show()\n",
    "res=df.distinct().count()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2ae70847-c3cd-4751-8cb1-5d411d2f17c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+--------------------+--------------------+-----------------+--------------------+-------------+-----------------+--------------------+----+--------------+--------------------+-------------+\n",
      "|_corrupt_record|                 _id|  amazon_product_url|              author| bestsellers_date|         description|        price|   published_date|           publisher|rank|rank_last_week|               title|weeks_on_list|\n",
      "+---------------+--------------------+--------------------+--------------------+-----------------+--------------------+-------------+-----------------+--------------------+----+--------------+--------------------+-------------+\n",
      "|            man|{5b4aa4ead3089013...|http://www.amazon...|       Dean R Koontz|{{1211587200000}}|Odd Thomas, who c...|   {NULL, 27}|{{1212883200000}}|              Bantam| {1}|           {0}|           ODD HOURS|          {1}|\n",
      "|            man|{5b4aa4ead3089013...|http://www.amazon...|     Stephenie Meyer|{{1211587200000}}|Aliens have taken...|{25.99, NULL}|{{1212883200000}}|       Little, Brown| {2}|           {1}|            THE HOST|          {3}|\n",
      "|            man|{5b4aa4ead3089013...|http://www.amazon...|        Emily Giffin|{{1211587200000}}|A woman's happy m...|{24.95, NULL}|{{1212883200000}}|        St. Martin's| {3}|           {2}|LOVE THE ONE YOU'...|          {2}|\n",
      "|            man|{5b4aa4ead3089013...|http://www.amazon...|   Patricia Cornwell|{{1211587200000}}|A Massachusetts s...|{22.95, NULL}|{{1212883200000}}|              Putnam| {4}|           {0}|           THE FRONT|          {1}|\n",
      "|            man|{5b4aa4ead3089013...|http://www.amazon...|     Chuck Palahniuk|{{1211587200000}}|An aging porn que...|{24.95, NULL}|{{1212883200000}}|           Doubleday| {5}|           {0}|               SNUFF|          {1}|\n",
      "|            man|{5b4aa4ead3089013...|http://www.amazon...|James Patterson a...|{{1211587200000}}|A woman finds an ...|{24.99, NULL}|{{1212883200000}}|       Little, Brown| {6}|           {3}|SUNDAYS AT TIFFANYS|          {4}|\n",
      "|            man|{5b4aa4ead3089013...|http://www.amazon...|       John Sandford|{{1211587200000}}|The Minneapolis d...|{26.95, NULL}|{{1212883200000}}|              Putnam| {7}|           {4}|        PHANTOM PREY|          {3}|\n",
      "|            man|{5b4aa4ead3089013...|http://www.amazon...|       Jimmy Buffett|{{1211587200000}}|A Southern family...|{21.99, NULL}|{{1212883200000}}|       Little, Brown| {8}|           {6}|          SWINE NOT?|          {2}|\n",
      "|            man|{5b4aa4ead3089013...|http://www.amazon...|    Elizabeth George|{{1211587200000}}|In Cornwall, tryi...|{27.95, NULL}|{{1212883200000}}|              Harper| {9}|           {8}|     CARELESS IN RED|          {3}|\n",
      "|            man|{5b4aa4ead3089013...|http://www.amazon...|      David Baldacci|{{1211587200000}}|An intelligence a...|{26.99, NULL}|{{1212883200000}}|       Grand Central|{10}|           {7}|     THE WHOLE TRUTH|          {5}|\n",
      "|            man|{5b4aa4ead3089013...|http://www.amazon...|        Troy Denning|{{1211587200000}}|The New Jedi orde...|   {NULL, 27}|{{1212883200000}}|  Del Rey/Ballantine|{11}|           {5}|          INVINCIBLE|          {2}|\n",
      "|            man|{5b4aa4ead3089013...|http://www.amazon...|          James Frey|{{1211587200000}}|A novel, set in L...|{26.95, NULL}|{{1212883200000}}|              Harper|{12}|           {9}|BRIGHT SHINY MORNING|          {2}|\n",
      "|            man|{5b4aa4ead3089013...|http://www.amazon...|         Garth Stein|{{1211587200000}}|A Lab-terrier mix...|{23.95, NULL}|{{1212883200000}}|              Harper|{13}|           {0}|THE ART OF RACING...|          {1}|\n",
      "|            man|{5b4aa4ead3089013...|http://www.amazon...|     Debbie Macomber|{{1211587200000}}|A widow who owns ...|{24.95, NULL}|{{1212883200000}}|                Mira|{14}|          {10}|       TWENTY WISHES|          {4}|\n",
      "|            man|{5b4aa4ead3089013...|http://www.amazon...|         Jeff Shaara|{{1211587200000}}|A novel about the...|   {NULL, 28}|{{1212883200000}}|          Ballantine|{15}|          {11}|      THE STEEL WAVE|          {2}|\n",
      "|            man|{5b4aa4ead3089013...|http://www.amazon...|    Phillip Margolin|{{1211587200000}}|                    |    {NULL, 0}|{{1212883200000}}|HarperCollins Pub...|{16}|           {0}| EXECUTIVE PRIVILEGE|          {0}|\n",
      "|            man|{5b4aa4ead3089013...|http://www.amazon...|       Jhumpa Lahiri|{{1211587200000}}|Stories of the an...|    {NULL, 0}|{{1212883200000}}|               Knopf|{17}|           {0}|  UNACCUSTOMED EARTH|          {0}|\n",
      "|            man|{5b4aa4ead3089013...|http://www.amazon...|      Joseph O'Neill|{{1211587200000}}|A Dutchman desert...|    {NULL, 0}|{{1212883200000}}|Knopf Publishing ...|{18}|           {0}|          NETHERLAND|          {0}|\n",
      "|            man|{5b4aa4ead3089013...|http://www.amazon...|        John Grisham|{{1211587200000}}|Political and leg...|    {NULL, 0}|{{1212883200000}}|Doubleday Publishing|{19}|           {0}|          THE APPEAL|          {0}|\n",
      "|            man|{5b4aa4ead3089013...|http://www.amazon...|       James Rollins|{{1211587200000}}|                    |    {NULL, 0}|{{1212883200000}}|Random House Publ...|{20}|           {0}|INDIANA JONES AND...|          {0}|\n",
      "+---------------+--------------------+--------------------+--------------------+-----------------+--------------------+-------------+-----------------+--------------------+----+--------------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sql task6\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('apli').getOrCreate()\n",
    "df=spark.read.json('Dataframe_sql.json')\n",
    "res=df.fillna('man') #for entire table\n",
    "# res=df.fillna(0,subset=['Open'])#for specific col\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8698ce2b-2baf-41b4-bef4-e97db120781b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "| ram| 23|\n",
      "|sham| 25|\n",
      "|unkn| 35|\n",
      "|unkn| 24|\n",
      "+----+---+\n",
      "\n",
      "\n",
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|    ram| 23|\n",
      "|   sham| 25|\n",
      "|manmith| 35|\n",
      "|manmith| 24|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# task7 replacing one val with another\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when,col \n",
    "spark=SparkSession.builder.appName('apli').getOrCreate()\n",
    "data=[('ram',23),('sham',25),('unkn',35),('unkn',24)]\n",
    "col=['name','age']\n",
    "df=spark.createDataFrame(data,col)\n",
    "df.show()\n",
    "print()\n",
    "# res=df.withColumn('name',when(df.name=='unkn','anonymous').otherwise('name'))\n",
    "# res=df.replace('unkn','manmith','name')\n",
    "# res=df.withColumn('name',when((df.name=='unkn')&(df.age==35),'manmith').otherwise(df.name))\n",
    "res=df.replace('unkn','manmith')\n",
    "\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "456d9f07-f522-4bc5-aab0-9909aeab9215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----+-------------+\n",
      "|               title|              author|rank|        price|\n",
      "+--------------------+--------------------+----+-------------+\n",
      "|           ODD HOURS|       Dean R Koontz| {1}|   {NULL, 27}|\n",
      "|            THE HOST|     Stephenie Meyer| {2}|{25.99, NULL}|\n",
      "|LOVE THE ONE YOU'...|        Emily Giffin| {3}|{24.95, NULL}|\n",
      "|           THE FRONT|   Patricia Cornwell| {4}|{22.95, NULL}|\n",
      "|               SNUFF|     Chuck Palahniuk| {5}|{24.95, NULL}|\n",
      "|SUNDAYS AT TIFFANYS|James Patterson a...| {6}|{24.99, NULL}|\n",
      "|        PHANTOM PREY|       John Sandford| {7}|{26.95, NULL}|\n",
      "|          SWINE NOT?|       Jimmy Buffett| {8}|{21.99, NULL}|\n",
      "|     CARELESS IN RED|    Elizabeth George| {9}|{27.95, NULL}|\n",
      "|     THE WHOLE TRUTH|      David Baldacci|{10}|{26.99, NULL}|\n",
      "|          INVINCIBLE|        Troy Denning|{11}|   {NULL, 27}|\n",
      "|BRIGHT SHINY MORNING|          James Frey|{12}|{26.95, NULL}|\n",
      "|THE ART OF RACING...|         Garth Stein|{13}|{23.95, NULL}|\n",
      "|       TWENTY WISHES|     Debbie Macomber|{14}|{24.95, NULL}|\n",
      "|      THE STEEL WAVE|         Jeff Shaara|{15}|   {NULL, 28}|\n",
      "| EXECUTIVE PRIVILEGE|    Phillip Margolin|{16}|    {NULL, 0}|\n",
      "|  UNACCUSTOMED EARTH|       Jhumpa Lahiri|{17}|    {NULL, 0}|\n",
      "|          NETHERLAND|      Joseph O'Neill|{18}|    {NULL, 0}|\n",
      "|          THE APPEAL|        John Grisham|{19}|    {NULL, 0}|\n",
      "|INDIANA JONES AND...|       James Rollins|{20}|    {NULL, 0}|\n",
      "+--------------------+--------------------+----+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#task8\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('apli').getOrCreate()\n",
    "df=spark.read.json('Dataframe_sql.json')\n",
    "res=df.select('title','author','rank','price')\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "b0f64f00-acbc-4da1-ad07-4861e3f543d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+-----+\n",
      "|title                                             |label|\n",
      "+--------------------------------------------------+-----+\n",
      "|ODD HOURS                                         |1    |\n",
      "|THE HOST                                          |0    |\n",
      "|LOVE THE ONE YOU'RE WITH                          |0    |\n",
      "|THE FRONT                                         |0    |\n",
      "|SNUFF                                             |0    |\n",
      "|SUNDAYS AT TIFFANYS                              |0    |\n",
      "|PHANTOM PREY                                      |0    |\n",
      "|SWINE NOT?                                        |0    |\n",
      "|CARELESS IN RED                                   |0    |\n",
      "|THE WHOLE TRUTH                                   |0    |\n",
      "|INVINCIBLE                                        |0    |\n",
      "|BRIGHT SHINY MORNING                              |0    |\n",
      "|THE ART OF RACING IN THE RAIN                     |0    |\n",
      "|TWENTY WISHES                                     |0    |\n",
      "|THE STEEL WAVE                                    |0    |\n",
      "|EXECUTIVE PRIVILEGE                               |0    |\n",
      "|UNACCUSTOMED EARTH                                |0    |\n",
      "|NETHERLAND                                        |0    |\n",
      "|THE APPEAL                                        |0    |\n",
      "|INDIANA JONES AND THE KINGDOM OF THE CRYSTAL SKULL|0    |\n",
      "+--------------------------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sql task9\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TitleClassifier\").getOrCreate()\n",
    "\n",
    "df = spark.read.json(\"Dataframe_sql.json\")\n",
    "condition=(col('title')=='ODD HOURS').cast('int')\n",
    "res=df.withColumn('label',condition)\n",
    "res1=res.select('title','label')\n",
    "res1.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "b7cd473c-10f2-46a4-bdd7-35ddb6b84eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+-----+\n",
      "|title                                             |label|\n",
      "+--------------------------------------------------+-----+\n",
      "|ODD HOURS                                         |1    |\n",
      "|THE HOST                                          |0    |\n",
      "|LOVE THE ONE YOU'RE WITH                          |0    |\n",
      "|THE FRONT                                         |0    |\n",
      "|SNUFF                                             |0    |\n",
      "|SUNDAYS AT TIFFANYS                              |0    |\n",
      "|PHANTOM PREY                                      |0    |\n",
      "|SWINE NOT?                                        |0    |\n",
      "|CARELESS IN RED                                   |0    |\n",
      "|THE WHOLE TRUTH                                   |0    |\n",
      "|INVINCIBLE                                        |0    |\n",
      "|BRIGHT SHINY MORNING                              |0    |\n",
      "|THE ART OF RACING IN THE RAIN                     |0    |\n",
      "|TWENTY WISHES                                     |0    |\n",
      "|THE STEEL WAVE                                    |0    |\n",
      "|EXECUTIVE PRIVILEGE                               |0    |\n",
      "|UNACCUSTOMED EARTH                                |0    |\n",
      "|NETHERLAND                                        |0    |\n",
      "|THE APPEAL                                        |0    |\n",
      "|INDIANA JONES AND THE KINGDOM OF THE CRYSTAL SKULL|0    |\n",
      "+--------------------------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,when\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"TitleClassifier\").getOrCreate()\n",
    "\n",
    "# Read JSON data\n",
    "df = spark.read.json(\"Dataframe_sql.json\")\n",
    "\n",
    "res=df.withColumn('label',when(col('title')=='ODD HOURS',1).otherwise(0))\n",
    "res1=res.select('title','label')\n",
    "res1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e68d7e4a-4815-4cd5-9a23-5aa858ecade8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------------+---------------------------------------------------------------------------------+-------------+-----------------+------------------------------------------------------------------------------------------+-------------+-----------------+------------+----+--------------+------------------------+-------------+\n",
      "|_corrupt_record|_id                       |amazon_product_url                                                               |author       |bestsellers_date |description                                                                               |price        |published_date   |publisher   |rank|rank_last_week|title                   |weeks_on_list|\n",
      "+---------------+--------------------------+---------------------------------------------------------------------------------+-------------+-----------------+------------------------------------------------------------------------------------------+-------------+-----------------+------------+----+--------------+------------------------+-------------+\n",
      "|NULL           |{5b4aa4ead3089013507db18d}|http://www.amazon.com/Love-Youre-With-Emily-Giffin/dp/0312348665?tag=NYTBS-20    |Emily Giffin |{{1211587200000}}|A woman's happy marriage is shaken when she encounters an old boyfriend.                  |{24.95, NULL}|{{1212883200000}}|St. Martin's|{3} |{2}           |LOVE THE ONE YOU'RE WITH|{2}          |\n",
      "|NULL           |{5b4aa4ead3089013507db191}|http://www.amazon.com/Phantom-Prey-John-Sandford/dp/0425227987?tag=NYTBS-20      |John Sandford|{{1211587200000}}|The Minneapolis detective Lucas Davenport investigates a string of murders of young Goths.|{26.95, NULL}|{{1212883200000}}|Putnam      |{7} |{4}           |PHANTOM PREY            |{3}          |\n",
      "|NULL           |{5b4aa4ead3089013507db1a2}|http://www.amazon.com/Love-Youre-With-Emily-Giffin/dp/0312348665?tag=NYTBS-20    |Emily Giffin |{{1212192000000}}|A womans happy marriage is shaken when she encounters an old boyfriend.                  |{24.95, NULL}|{{1213488000000}}|St. Martins|{4} |{3}           |LOVE THE ONE YOU'RE WITH|{3}          |\n",
      "|NULL           |{5b4aa4ead3089013507db1a7}|http://www.amazon.com/Phantom-Prey-John-Sandford/dp/0425227987?tag=NYTBS-20      |John Sandford|{{1212192000000}}|The Minneapolis detective Lucas Davenport investigates a string of murders of young Goths.|{26.95, NULL}|{{1213488000000}}|Putnam      |{9} |{7}           |PHANTOM PREY            |{4}          |\n",
      "|NULL           |{5b4aa4ead3089013507db1b6}|http://www.amazon.com/Love-Youre-With-Emily-Giffin/dp/0312348665?tag=NYTBS-20    |Emily Giffin |{{1212796800000}}|A womans happy marriage is shaken when she encounters an old boyfriend.                  |{24.95, NULL}|{{1214092800000}}|St. Martins|{4} |{4}           |LOVE THE ONE YOU'RE WITH|{4}          |\n",
      "|NULL           |{5b4aa4ead3089013507db1be}|http://www.amazon.com/Phantom-Prey-John-Sandford/dp/0425227987?tag=NYTBS-20      |John Sandford|{{1212796800000}}|The Minneapolis detective Lucas Davenport investigates a string of murders of young Goths.|{26.95, NULL}|{{1214092800000}}|Putnam      |{12}|{9}           |PHANTOM PREY            |{5}          |\n",
      "|NULL           |{5b4aa4ead3089013507db1cb}|http://www.amazon.com/Love-Youre-With-Emily-Giffin/dp/0312348665?tag=NYTBS-20    |Emily Giffin |{{1213401600000}}|A womans happy marriage is shaken when she encounters an old boyfriend.                  |{24.95, NULL}|{{1214697600000}}|St. Martins|{5} |{4}           |LOVE THE ONE YOU'RE WITH|{5}          |\n",
      "|NULL           |{5b4aa4ead3089013507db1df}|http://www.amazon.com/Love-Youre-With-Emily-Giffin/dp/0312348665?tag=NYTBS-20    |Emily Giffin |{{1214006400000}}|A womans happy marriage is shaken.                                                       |{24.95, NULL}|{{1215302400000}}|St. Martins|{5} |{5}           |LOVE THE ONE YOU'RE WITH|{6}          |\n",
      "|NULL           |{5b4aa4ead3089013507db1ee}|http://www.amazon.com/Phantom-Prey-John-Sandford/dp/0425227987?tag=NYTBS-20      |John Sandford|{{1214006400000}}|The Minneapolis detective Lucas Davenport investigates a string of murders of young Goths.|{NULL, 0}    |{{1215302400000}}|Putnam      |{20}|{0}           |PHANTOM PREY            |{0}          |\n",
      "|NULL           |{5b4aa4ead3089013507db1f8}|http://www.amazon.com/Love-Youre-With-Emily-Giffin/dp/0312348665?tag=NYTBS-20    |Emily Giffin |{{1214611200000}}|A womans happy marriage is shaken when she encounters an old boyfriend.                  |{24.95, NULL}|{{1215907200000}}|St. Martins|{10}|{5}           |LOVE THE ONE YOU'RE WITH|{7}          |\n",
      "|NULL           |{5b4aa4ead3089013507db20d}|http://www.amazon.com/Love-Youre-With-Emily-Giffin/dp/0312348665?tag=NYTBS-20    |Emily Giffin |{{1215216000000}}|A womans happy marriage is shaken when she encounters an old boyfriend.                  |{24.95, NULL}|{{1216512000000}}|St. Martins|{11}|{10}          |LOVE THE ONE YOU'RE WITH|{8}          |\n",
      "|NULL           |{5b4aa4ead3089013507db220}|http://www.amazon.com/Love-Youre-With-Emily-Giffin/dp/0312348665?tag=NYTBS-20    |Emily Giffin |{{1215820800000}}|A womans happy marriage is shaken when she encounters an old boyfriend.                  |{24.95, NULL}|{{1217116800000}}|St. Martins|{10}|{11}          |LOVE THE ONE YOU'RE WITH|{9}          |\n",
      "|NULL           |{5b4aa4ead3089013507db237}|http://www.amazon.com/Love-Youre-With-Emily-Giffin/dp/0312348665?tag=NYTBS-20    |Emily Giffin |{{1216425600000}}|A womans happy marriage is shaken when she encounters an old boyfriend.                  |{24.95, NULL}|{{1217721600000}}|St. Martins|{13}|{10}          |LOVE THE ONE YOU'RE WITH|{10}         |\n",
      "|NULL           |{5b4aa4ead3089013507db24d}|http://www.amazon.com/Love-Youre-With-Emily-Giffin/dp/0312348665?tag=NYTBS-20    |Emily Giffin |{{1217030400000}}|A womans happy marriage is shaken when she encounters an old boyfriend.                  |{24.95, NULL}|{{1218326400000}}|St. Martins|{15}|{13}          |LOVE THE ONE YOU'RE WITH|{11}         |\n",
      "|NULL           |{5b4aa4ead3089013507db260}|http://www.amazon.com/Love-Youre-With-Emily-Giffin/dp/0312348665?tag=NYTBS-20    |Emily Giffin |{{1217635200000}}|A woman encounters an old boyfriend.                                                      |{24.95, NULL}|{{1218931200000}}|St. Martins|{14}|{15}          |LOVE THE ONE YOU'RE WITH|{12}         |\n",
      "|NULL           |{5b4aa4ead3089013507db273}|http://www.amazon.com/Love-Youre-With-Emily-Giffin/dp/0312348665?tag=NYTBS-20    |Emily Giffin |{{1218240000000}}|A woman encounters an old boyfriend.                                                      |{24.95, NULL}|{{1219536000000}}|St. Martins|{13}|{14}          |LOVE THE ONE YOU'RE WITH|{13}         |\n",
      "|NULL           |{5b4aa4ead3089013507db289}|http://www.amazon.com/Love-Youre-With-Emily-Giffin/dp/0312348665?tag=NYTBS-20    |Emily Giffin |{{1218844800000}}|A woman encounters an old boyfriend.                                                      |{24.95, NULL}|{{1220140800000}}|St. Martins|{15}|{13}          |LOVE THE ONE YOU'RE WITH|{14}         |\n",
      "|NULL           |{5b4aa4ead3089013507db2a0}|http://www.amazon.com/Love-Youre-With-Emily-Giffin/dp/0312348665?tag=NYTBS-20    |Emily Giffin |{{1219449600000}}|A woman's happy marriage is shaken when she encounters an old boyfriend.                  |{NULL, 0}    |{{1220745600000}}|St. Martins|{18}|{0}           |LOVE THE ONE YOU'RE WITH|{0}          |\n",
      "|NULL           |{5b4aa4ead3089013507db2f4}|http://www.amazon.com/Heat-Lightning-Virgil-Flowers-No/dp/0425230619?tag=NYTBS-20|John Sandford|{{1222473600000}}|Virgil Flowers investigates murder cases linked by a lemon in the mouth of each victim.   |{26.95, NULL}|{{1223769600000}}|Putnam      |{2} |{0}           |HEAT LIGHTNING          |{1}          |\n",
      "|NULL           |{5b4aa4ead3089013507db30a}|http://www.amazon.com/Heat-Lightning-Virgil-Flowers-No/dp/0425230619?tag=NYTBS-20|John Sandford|{{1223078400000}}|Virgil Flowers investigates murder cases linked by a lemon in the mouth of each victim.   |{26.95, NULL}|{{1224374400000}}|Putnam      |{4} |{2}           |HEAT LIGHTNING          |{2}          |\n",
      "+---------------+--------------------------+---------------------------------------------------------------------------------+-------------+-----------------+------------------------------------------------------------------------------------------+-------------+-----------------+------------+----+--------------+------------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# task10\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "spark=SparkSession.builder.appName('apli').getOrCreate()\n",
    "df=spark.read.json('Dataframe_sql.json')\n",
    "# res=df.filter(df.author.isin(['John Sandford', 'Emily Giffin']))\n",
    "res=df.filter(col('author').isin(['John Sandford', 'Emily Giffin']))\n",
    "res.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d271fb21-0332-4d64-9314-18f09e9e8ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------------------------------------+\n",
      "|author           |title                                             |\n",
      "+-----------------+--------------------------------------------------+\n",
      "|Stephenie Meyer  |THE HOST                                          |\n",
      "|Emily Giffin     |LOVE THE ONE YOU'RE WITH                          |\n",
      "|Patricia Cornwell|THE FRONT                                         |\n",
      "|David Baldacci   |THE WHOLE TRUTH                                   |\n",
      "|Garth Stein      |THE ART OF RACING IN THE RAIN                     |\n",
      "|Jeff Shaara      |THE STEEL WAVE                                    |\n",
      "|Joseph O'Neill   |NETHERLAND                                        |\n",
      "|John Grisham     |THE APPEAL                                        |\n",
      "|James Rollins    |INDIANA JONES AND THE KINGDOM OF THE CRYSTAL SKULL|\n",
      "|Stephenie Meyer  |THE HOST                                          |\n",
      "|Emily Giffin     |LOVE THE ONE YOU'RE WITH                          |\n",
      "|Patricia Cornwell|THE FRONT                                         |\n",
      "|David Baldacci   |THE WHOLE TRUTH                                   |\n",
      "|Garth Stein      |THE ART OF RACING IN THE RAIN                     |\n",
      "|Salman Rushdie   |THE ENCHANTRESS OF FLORENCE                       |\n",
      "|Stephenie Meyer  |THE HOST                                          |\n",
      "|Emily Giffin     |LOVE THE ONE YOU'RE WITH                          |\n",
      "|Patricia Cornwell|THE FRONT                                         |\n",
      "|Garth Stein      |THE ART OF RACING IN THE RAIN                     |\n",
      "|Alan Furst       |THE SPIES OF WARSAW                               |\n",
      "+-----------------+--------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# task11\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "spark=SparkSession.builder.appName('apli').getOrCreate()\n",
    "df=spark.read.json('Dataframe_sql.json')\n",
    "res = df.filter(df.title.like(\"%THE%\"))\n",
    "\n",
    "# Select and display the relevant columns\n",
    "result = res.select(\"author\", \"title\")\n",
    "result.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5492581-effb-4bc2-bc59-49399f533586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------------------+\n",
      "|author           |title                        |\n",
      "+-----------------+-----------------------------+\n",
      "|Stephenie Meyer  |THE HOST                     |\n",
      "|Patricia Cornwell|THE FRONT                    |\n",
      "|David Baldacci   |THE WHOLE TRUTH              |\n",
      "|Garth Stein      |THE ART OF RACING IN THE RAIN|\n",
      "|Jeff Shaara      |THE STEEL WAVE               |\n",
      "|John Grisham     |THE APPEAL                   |\n",
      "|Stephenie Meyer  |THE HOST                     |\n",
      "|Patricia Cornwell|THE FRONT                    |\n",
      "|David Baldacci   |THE WHOLE TRUTH              |\n",
      "|Garth Stein      |THE ART OF RACING IN THE RAIN|\n",
      "|Salman Rushdie   |THE ENCHANTRESS OF FLORENCE  |\n",
      "|Stephenie Meyer  |THE HOST                     |\n",
      "|Patricia Cornwell|THE FRONT                    |\n",
      "|Garth Stein      |THE ART OF RACING IN THE RAIN|\n",
      "|Alan Furst       |THE SPIES OF WARSAW          |\n",
      "|Andre Dubus      |THE GARDEN OF LAST DAYS      |\n",
      "|Salman Rushdie   |THE ENCHANTRESS OF FLORENCE  |\n",
      "|Stephenie Meyer  |THE HOST                     |\n",
      "|Jeffery Deaver   |THE BROKEN WINDOW            |\n",
      "|Garth Stein      |THE ART OF RACING IN THE RAIN|\n",
      "+-----------------+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# task12\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "spark=SparkSession.builder.appName('apli').getOrCreate()\n",
    "df=spark.read.json('Dataframe_sql.json')\n",
    "res=df.filter(df.title.like('THE%'))\n",
    "res1=res.select('author','title')\n",
    "res1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0ea606b-c63b-422f-898f-68d2864811e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n",
      "|author           |title              |\n",
      "+-----------------+-------------------+\n",
      "|Patricia Cornwell|THE FRONT          |\n",
      "|Patricia Cornwell|THE FRONT          |\n",
      "|Patricia Cornwell|THE FRONT          |\n",
      "|Daniel Silva     |THE SECRET SERVANT |\n",
      "|Daniel Silva     |THE SECRET SERVANT |\n",
      "|Daniel Silva     |THE SECRET SERVANT |\n",
      "|Daniel Silva     |THE SECRET SERVANT |\n",
      "|Daniel Silva     |THE SECRET SERVANT |\n",
      "|Daniel Silva     |THE SECRET SERVANT |\n",
      "|Daniel Silva     |THE SECRET SERVANT |\n",
      "|P D James        |THE PRIVATE PATIENT|\n",
      "|P D James        |THE PRIVATE PATIENT|\n",
      "|P D James        |THE PRIVATE PATIENT|\n",
      "|J A Jance        |CRUEL INTENT       |\n",
      "|P D James        |THE PRIVATE PATIENT|\n",
      "|P D James        |THE PRIVATE PATIENT|\n",
      "|P D James        |THE PRIVATE PATIENT|\n",
      "+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#task13\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "spark=SparkSession.builder.appName('apli').getOrCreate()\n",
    "df=spark.read.json('Dataframe_sql.json')\n",
    "# res=df.filter(df.title.like('%NT'))\n",
    "res=df.filter(df.title.endswith(\"NT\"))\n",
    "res1=res.select('author','title')\n",
    "res1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ccc25613-f712-42e5-906f-7a830d54737b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+-------+-------+-------+\n",
      "|author                                  |title_1|title_2|title_3|\n",
      "+----------------------------------------+-------+-------+-------+\n",
      "|Dean R Koontz                           |Dea    |an R K |Dean R |\n",
      "|Stephenie Meyer                         |Ste    |epheni |Stephe |\n",
      "|Emily Giffin                            |Emi    |ily Gi |Emily  |\n",
      "|Patricia Cornwell                       |Pat    |tricia |Patric |\n",
      "|Chuck Palahniuk                         |Chu    |uck Pa |Chuck  |\n",
      "|James Patterson and Gabrielle Charbonnet|Jam    |mes Pa |James  |\n",
      "|John Sandford                           |Joh    |hn San |John S |\n",
      "|Jimmy Buffett                           |Jim    |mmy Bu |Jimmy  |\n",
      "|Elizabeth George                        |Eli    |izabet |Elizab |\n",
      "|David Baldacci                          |Dav    |vid Ba |David  |\n",
      "|Troy Denning                            |Tro    |oy Den |Troy D |\n",
      "|James Frey                              |Jam    |mes Fr |James  |\n",
      "|Garth Stein                             |Gar    |rth St |Garth  |\n",
      "|Debbie Macomber                         |Deb    |bbie M |Debbie |\n",
      "|Jeff Shaara                             |Jef    |ff Sha |Jeff S |\n",
      "|Phillip Margolin                        |Phi    |illip  |Philli |\n",
      "|Jhumpa Lahiri                           |Jhu    |umpa L |Jhumpa |\n",
      "|Joseph O'Neill                          |Jos    |seph O |Joseph |\n",
      "|John Grisham                            |Joh    |hn Gri |John G |\n",
      "|James Rollins                           |Jam    |mes Ro |James  |\n",
      "+----------------------------------------+-------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#task14\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, substring\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"SubstringExtraction\").getOrCreate()\n",
    "\n",
    "# Create a DataFrame (Replace this with your DataFrame)\n",
    "# data = [(\"John Doe\",), (\"Alice Smith\",), (\"Bob Johnson\",)]\n",
    "# columns = [\"author\"]\n",
    "# df = spark.createDataFrame(data, columns)\n",
    "df=spark.read.json('Dataframe_sql.json')\n",
    "\n",
    "df = df.withColumn(\"title_1\", substring(col(\"author\"), 1, 3))\n",
    "df = df.withColumn(\"title_2\", substring(col(\"author\"), 3, 6))\n",
    "df = df.withColumn(\"title_3\", substring(col(\"author\"), 1, 6))\n",
    "res=df.select(\"author\",\"title_1\",\"title_2\",\"title_3\")\n",
    "# Show the resulting DataFrame\n",
    "res.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49a575f4-9c4e-481c-a5e7-7b0da3625c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------+\n",
      "|              author|               title|           new_col|\n",
      "+--------------------+--------------------+------------------+\n",
      "|       Dean R Koontz|           ODD HOURS|This is new column|\n",
      "|     Stephenie Meyer|            THE HOST|This is new column|\n",
      "|        Emily Giffin|LOVE THE ONE YOU'...|This is new column|\n",
      "|   Patricia Cornwell|           THE FRONT|This is new column|\n",
      "|     Chuck Palahniuk|               SNUFF|This is new column|\n",
      "|James Patterson a...|SUNDAYS AT TIFFANYS|This is new column|\n",
      "|       John Sandford|        PHANTOM PREY|This is new column|\n",
      "|       Jimmy Buffett|          SWINE NOT?|This is new column|\n",
      "|    Elizabeth George|     CARELESS IN RED|This is new column|\n",
      "|      David Baldacci|     THE WHOLE TRUTH|This is new column|\n",
      "|        Troy Denning|          INVINCIBLE|This is new column|\n",
      "|          James Frey|BRIGHT SHINY MORNING|This is new column|\n",
      "|         Garth Stein|THE ART OF RACING...|This is new column|\n",
      "|     Debbie Macomber|       TWENTY WISHES|This is new column|\n",
      "|         Jeff Shaara|      THE STEEL WAVE|This is new column|\n",
      "|    Phillip Margolin| EXECUTIVE PRIVILEGE|This is new column|\n",
      "|       Jhumpa Lahiri|  UNACCUSTOMED EARTH|This is new column|\n",
      "|      Joseph O'Neill|          NETHERLAND|This is new column|\n",
      "|        John Grisham|          THE APPEAL|This is new column|\n",
      "|       James Rollins|INDIANA JONES AND...|This is new column|\n",
      "+--------------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#task15\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "spark=SparkSession.builder.appName('app').getOrCreate()\n",
    "df=spark.read.json('Dataframe_sql.json')\n",
    "res=df.withColumn('new_col',lit('This is new column'))\n",
    "res1=res.select(['author','title','new_col'])\n",
    "res1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ed17083-26e2-436f-8749-921d57eb2fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|              author|               title|                 URL|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|       Dean R Koontz|           ODD HOURS|http://www.amazon...|\n",
      "|     Stephenie Meyer|            THE HOST|http://www.amazon...|\n",
      "|        Emily Giffin|LOVE THE ONE YOU'...|http://www.amazon...|\n",
      "|   Patricia Cornwell|           THE FRONT|http://www.amazon...|\n",
      "|     Chuck Palahniuk|               SNUFF|http://www.amazon...|\n",
      "|James Patterson a...|SUNDAYS AT TIFFANYS|http://www.amazon...|\n",
      "|       John Sandford|        PHANTOM PREY|http://www.amazon...|\n",
      "|       Jimmy Buffett|          SWINE NOT?|http://www.amazon...|\n",
      "|    Elizabeth George|     CARELESS IN RED|http://www.amazon...|\n",
      "|      David Baldacci|     THE WHOLE TRUTH|http://www.amazon...|\n",
      "|        Troy Denning|          INVINCIBLE|http://www.amazon...|\n",
      "|          James Frey|BRIGHT SHINY MORNING|http://www.amazon...|\n",
      "|         Garth Stein|THE ART OF RACING...|http://www.amazon...|\n",
      "|     Debbie Macomber|       TWENTY WISHES|http://www.amazon...|\n",
      "|         Jeff Shaara|      THE STEEL WAVE|http://www.amazon...|\n",
      "|    Phillip Margolin| EXECUTIVE PRIVILEGE|http://www.amazon...|\n",
      "|       Jhumpa Lahiri|  UNACCUSTOMED EARTH|http://www.amazon...|\n",
      "|      Joseph O'Neill|          NETHERLAND|http://www.amazon...|\n",
      "|        John Grisham|          THE APPEAL|http://www.amazon...|\n",
      "|       James Rollins|INDIANA JONES AND...|http://www.amazon...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# task16\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.appName('app').getOrCreate()\n",
    "df=spark.read.json('Dataframe_sql.json')\n",
    "res=df.withColumnRenamed('amazon_product_url','URL')\n",
    "res1=res.select(['author','title','URL'])\n",
    "res1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e22bff03-b339-493a-8014-0600a83b9c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+--------------------------------------------------+\n",
      "|author                                  |title                                             |\n",
      "+----------------------------------------+--------------------------------------------------+\n",
      "|Dean R Koontz                           |ODD HOURS                                         |\n",
      "|Stephenie Meyer                         |THE HOST                                          |\n",
      "|Emily Giffin                            |LOVE THE ONE YOU'RE WITH                          |\n",
      "|Patricia Cornwell                       |THE FRONT                                         |\n",
      "|Chuck Palahniuk                         |SNUFF                                             |\n",
      "|James Patterson and Gabrielle Charbonnet|SUNDAYS AT TIFFANYS                              |\n",
      "|John Sandford                           |PHANTOM PREY                                      |\n",
      "|Jimmy Buffett                           |SWINE NOT?                                        |\n",
      "|Elizabeth George                        |CARELESS IN RED                                   |\n",
      "|David Baldacci                          |THE WHOLE TRUTH                                   |\n",
      "|Troy Denning                            |INVINCIBLE                                        |\n",
      "|James Frey                              |BRIGHT SHINY MORNING                              |\n",
      "|Garth Stein                             |THE ART OF RACING IN THE RAIN                     |\n",
      "|Debbie Macomber                         |TWENTY WISHES                                     |\n",
      "|Jeff Shaara                             |THE STEEL WAVE                                    |\n",
      "|Phillip Margolin                        |EXECUTIVE PRIVILEGE                               |\n",
      "|Jhumpa Lahiri                           |UNACCUSTOMED EARTH                                |\n",
      "|Joseph O'Neill                          |NETHERLAND                                        |\n",
      "|John Grisham                            |THE APPEAL                                        |\n",
      "|James Rollins                           |INDIANA JONES AND THE KINGDOM OF THE CRYSTAL SKULL|\n",
      "+----------------------------------------+--------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#task17\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"RemoveColumn\").getOrCreate()\n",
    "\n",
    "# Read JSON data\n",
    "df = spark.read.json(\"Dataframe_sql.json\")\n",
    "\n",
    "# Add a new column named 'new_column' with a constant value\n",
    "df = df.withColumn(\"new_column\", lit(\"This is a new column\"))\n",
    "res1=df.drop('new_column')\n",
    "result = res1.select(\"author\", \"title\")\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14b3da86-1ba8-4536-a625-610268186064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+\n",
      "|author            |book_count|\n",
      "+------------------+----------+\n",
      "|James Frey        |2         |\n",
      "|Elin Hilderbrand  |2         |\n",
      "|Sharon Kay Penman |1         |\n",
      "|Patricia Cornwell |14        |\n",
      "|Andre Dubus       |1         |\n",
      "|Terry Brooks      |4         |\n",
      "|Daniel Silva      |7         |\n",
      "|Jackie Collins    |3         |\n",
      "|Linda Howard      |2         |\n",
      "|Anne Perry        |1         |\n",
      "|David Baldacci    |14        |\n",
      "|E Annie Proulx    |1         |\n",
      "|Dennis Lehane     |5         |\n",
      "|Randy Wayne White |3         |\n",
      "|Jonathan Kellerman|5         |\n",
      "|Lauren Willig     |1         |\n",
      "|Spencer Quinn     |2         |\n",
      "|Anne Bishop       |1         |\n",
      "|Christine Feehan  |3         |\n",
      "|Anita Shreve      |4         |\n",
      "+------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#task18\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"RemoveColumn\").getOrCreate()\n",
    "\n",
    "# Read JSON data\n",
    "df = spark.read.json(\"Dataframe_sql.json\")\n",
    "res=df.groupBy('author').agg(count('*').alias('book_count'))\n",
    "res.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eed8258c-c24c-45a2-951c-6fd002272051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "|author         |title   |\n",
      "+---------------+--------+\n",
      "|Stephenie Meyer|THE HOST|\n",
      "|Stephenie Meyer|THE HOST|\n",
      "|Stephenie Meyer|THE HOST|\n",
      "|Stephenie Meyer|THE HOST|\n",
      "|Stephenie Meyer|THE HOST|\n",
      "|Stephenie Meyer|THE HOST|\n",
      "|Stephenie Meyer|THE HOST|\n",
      "|Stephenie Meyer|THE HOST|\n",
      "|Stephenie Meyer|THE HOST|\n",
      "|Stephenie Meyer|THE HOST|\n",
      "|Stephenie Meyer|THE HOST|\n",
      "|Stephenie Meyer|THE HOST|\n",
      "|Stephenie Meyer|THE HOST|\n",
      "|Stephenie Meyer|THE HOST|\n",
      "|Stephenie Meyer|THE HOST|\n",
      "|Stephenie Meyer|THE HOST|\n",
      "|Stephenie Meyer|THE HOST|\n",
      "|Stephenie Meyer|THE HOST|\n",
      "|Stephenie Meyer|THE HOST|\n",
      "|Stephenie Meyer|THE HOST|\n",
      "+---------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#task19\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"FilterByTitle\").getOrCreate()\n",
    "\n",
    "df = spark.read.json(\"Dataframe_sql.json\")\n",
    "\n",
    "res1 = df.filter(df.title == \"THE HOST\")\n",
    "\n",
    "result = res1.select(\"author\", \"title\")\n",
    "result.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6d4dc6c-146e-49c6-8edd-96a95e75074a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "| John| 25|\n",
      "|Alice| 30|\n",
      "|  Bob| 22|\n",
      "+-----+---+\n",
      "\n",
      "Row(Name='John', Age=25)\n",
      "Row(Name='Alice', Age=30)\n",
      "Row(Name='Bob', Age=22)\n"
     ]
    }
   ],
   "source": [
    "#task20\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DataFrameToRDD\").getOrCreate()\n",
    "\n",
    "data = [(\"John\", 25), (\"Alice\", 30), (\"Bob\", 22)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.show()\n",
    "rdd1 = df.rdd\n",
    "\n",
    "# rdd1.foreach(print)\n",
    "rdd_ele=rdd1.collect()\n",
    "for i in rdd_ele:\n",
    "    print(i)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30c99844-1009-49a7-9cfe-a789c2128eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "Banana\n",
      "Orange\n",
      "type of rdd is: <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "#task21\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"RDDToString\").getOrCreate()\n",
    "\n",
    "# Create an RDD (Replace this with your RDD)\n",
    "data = [\"Apple\", \"Banana\", \"Orange\"]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "rdd_ele=rdd.collect()\n",
    "rdd_type=type(rdd_ele[0])\n",
    "for i in rdd_ele:\n",
    "    print(i)\n",
    "print('type of rdd is:',rdd_type)\n",
    "\n",
    "\n",
    "# # Convert RDD to a comma-separated string\n",
    "# rdd_as_string = ','.join(rdd.collect())\n",
    "\n",
    "# # Display the result\n",
    "# print(rdd_as_string)\n",
    "\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# # Initialize Spark session\n",
    "# spark = SparkSession.builder.appName(\"DataFrameToStringRDD\").getOrCreate()\n",
    "\n",
    "# # Create a sample DataFrame\n",
    "# data = [(\"John\", 25), (\"Alice\", 30), (\"Bob\", 22)]\n",
    "# columns = [\"Name\", \"Age\"]\n",
    "# df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# # Convert the DataFrame to an RDD of strings\n",
    "# string_rdd = df.rdd.map(lambda row: \",\".join(map(str, row)))\n",
    "\n",
    "# # Display the RDD of strings\n",
    "# # string_rdd.foreach(print)\n",
    "# rdd_ele=string_rdd.collect()\n",
    "# rdd_type=type(rdd_ele[0])\n",
    "# for i in rdd_ele:\n",
    "#     print(i)\n",
    "# print('type of rdd ele is:',rdd_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8400ab54-cbca-4f9e-8c69-4d369c2305d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Name  Age\n",
      "0   John   25\n",
      "1  Alice   30\n",
      "2    Bob   22\n"
     ]
    }
   ],
   "source": [
    "#task22\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DataFrameToPandas\").getOrCreate()\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = [(\"John\", 25), (\"Alice\", 30), (\"Bob\", 22)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Convert PySpark DataFrame to Pandas DataFrame\n",
    "pandas_df = df.toPandas()\n",
    "\n",
    "# Display the Pandas DataFrame\n",
    "print(pandas_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8e417d38-1afb-467a-bc45-0855d1fac7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+----------+-----+---------+\n",
      "|department|state|total_sal|\n",
      "+----------+-----+---------+\n",
      "|     Sales|   NY|   176000|\n",
      "|     Sales|   CA|    81000|\n",
      "|   Finance|   CA|   189000|\n",
      "|   Finance|   NY|   162000|\n",
      "| Marketing|   NY|    91000|\n",
      "| Marketing|   CA|    80000|\n",
      "+----------+-----+---------+\n",
      "\n",
      "\n",
      "+----------+-----+-----------+\n",
      "|department|state|sum(salary)|\n",
      "+----------+-----+-----------+\n",
      "|     Sales|   NY|     176000|\n",
      "|     Sales|   CA|      81000|\n",
      "|   Finance|   CA|     189000|\n",
      "|   Finance|   NY|     162000|\n",
      "| Marketing|   NY|      91000|\n",
      "| Marketing|   CA|      80000|\n",
      "+----------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (\"James\", \"Sales\", \"NY\", 90000, 34, 10000),\n",
    "    (\"Michael\", \"Sales\", \"NY\", 86000, 56, 20000),\n",
    "    (\"Robert\", \"Sales\", \"CA\", 81000, 30, 23000),\n",
    "    (\"Maria\", \"Finance\", \"CA\", 90000, 24, 23000),\n",
    "    (\"Raman\", \"Finance\", \"CA\", 99000, 40, 24000),\n",
    "    (\"Scott\", \"Finance\", \"NY\", 83000, 36, 19000),\n",
    "    (\"Jen\", \"Finance\", \"NY\", 79000, 53, 15000),\n",
    "    (\"Jeff\", \"Marketing\", \"CA\", 80000, 25, 18000),\n",
    "    (\"Kumar\", \"Marketing\", \"NY\", 91000, 50, 21000)\n",
    "]\n",
    "\n",
    "col = [\"employee_name\", \"department\", \"state\", \"salary\", \"age\", \"bonus\"]\n",
    "\n",
    "df = spark.createDataFrame(data, col)\n",
    "\n",
    "# Print the schema and show the DataFrame\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "r=df.withColumn('salary',df.salary.cast('int'))\n",
    "r.printSchema()\n",
    "res=r.groupby('department','state').agg(sum('salary').alias('total_sal'))\n",
    "res.show()\n",
    "print()\n",
    "res1=df.groupBy('department','state').sum('salary')\n",
    "res1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "043d85ca-562f-48b4-84f3-2b6c48aba7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "| id|        Name|\n",
      "+---+------------+\n",
      "|  1|  john jones|\n",
      "|  2|tracey smith|\n",
      "|  3| amy sanders|\n",
      "+---+------------+\n",
      "\n",
      "+---+-------------+\n",
      "| id|convertedname|\n",
      "+---+-------------+\n",
      "|  1|   John Jones|\n",
      "|  2| Tracey Smith|\n",
      "|  3|  Amy Sanders|\n",
      "+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using udf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col as spark_col,udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "spark=SparkSession.builder.appName('apli').getOrCreate()\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\")]\n",
    "col=[\"id\",\"Name\"]\n",
    "df=spark.createDataFrame(data,col)\n",
    "df.show()\n",
    "# def fun(str):\n",
    "#     res=''\n",
    "#     a=str.split()\n",
    "#     for i in a:\n",
    "#         res+=i[0:1].upper()+i[1:len(i)]\n",
    "#     return res\n",
    "def fun1(name):\n",
    "    res=' '.join(i.capitalize() for i in name.split())\n",
    "    return res\n",
    "convert_udf=udf(fun1)\n",
    "res=df.select(spark_col('id'),convert_udf(spark_col('name')).alias('convertedname'))\n",
    "res.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b6f4587b-32c7-415f-a03a-b64889170a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "| id|        Name|\n",
      "+---+------------+\n",
      "|  1|  john jones|\n",
      "|  2|tracey smith|\n",
      "|  3| amy sanders|\n",
      "+---+------------+\n",
      "\n",
      "+---+-------------+\n",
      "| id|convertedname|\n",
      "+---+-------------+\n",
      "|  1|   John Jones|\n",
      "|  2| Tracey Smith|\n",
      "|  3|  Amy Sanders|\n",
      "+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col as spark_col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "spark = SparkSession.builder.appName('apli').getOrCreate()\n",
    "\n",
    "data = [(\"1\", \"john jones\"),\n",
    "        (\"2\", \"tracey smith\"),\n",
    "        (\"3\", \"amy sanders\")]\n",
    "\n",
    "columns = [\"id\", \"Name\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n",
    "# Rename the 'col' variable to avoid conflict with pyspark.sql.functions.col\n",
    "def fun(name):\n",
    "    return ' '.join(word.capitalize() for word in name.split())\n",
    "\n",
    "convert_udf = udf(fun, StringType())\n",
    "# res = df.select(spark_col('id'), convert_udf(spark_col('Name')).alias('ConvertedName'))\n",
    "res=df.select(df.id,convert_udf(df.Name).alias('convertedname'))\n",
    "\n",
    "\n",
    "res.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3b8a7da5-77e2-411c-97ee-0735d7a64d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "|id |Name        |\n",
      "+---+------------+\n",
      "|1  |john jones  |\n",
      "|2  |tracey smith|\n",
      "|3  |amy sanders |\n",
      "+---+------------+\n",
      "\n",
      "+---+------------+\n",
      "| id|updated_name|\n",
      "+---+------------+\n",
      "|  1|  John Jones|\n",
      "|  2|Tracey Smith|\n",
      "|  3| Amy Sanders|\n",
      "+---+------------+\n",
      "\n",
      "+---+------------+\n",
      "| id| upper(Name)|\n",
      "+---+------------+\n",
      "|  1|  JOHN JONES|\n",
      "|  2|TRACEY SMITH|\n",
      "|  3| AMY SANDERS|\n",
      "+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col as spark_col,udf,upper\n",
    "spark=SparkSession.builder.appName('app').getOrCreate()\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\")]\n",
    "col=[\"id\",\"Name\"]\n",
    "df=spark.createDataFrame(data,col)\n",
    "df.show(truncate=False)\n",
    "def fun(name):\n",
    "    res=' '.join(i.capitalize() for i in name.split())\n",
    "    return res\n",
    "create_udf=udf(fun)\n",
    "res=df.select(spark_col('id'),create_udf(spark_col('Name')).alias('updated_name'))\n",
    "res.show()\n",
    "res1=df.select(df.id,upper(df.Name))\n",
    "res1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "41133c3e-b15e-42bb-95d7-cd098558154c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|               NULL|   PR|     30100|\n",
      "|  2|    704|    NULL|PASEO COSTA DEL SUR|   PR|      NULL|\n",
      "|  3|    709|    NULL|       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|STANDARD|               NULL|   TX|      NULL|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|                  0|   PR|     30100|\n",
      "|  2|    704|    NULL|PASEO COSTA DEL SUR|   PR|      NULL|\n",
      "|  3|    709|    NULL|       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|STANDARD|                  0|   TX|      NULL|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- zipcode: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- city: string (nullable = false)\n",
      " |-- state: string (nullable = true)\n",
      " |-- population: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#fillna\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('api').getOrCreate()\n",
    "df=spark.read.csv('small_zipcode.csv',header=True,)\n",
    "df.show()\n",
    "# df.na.fill(value=0).show()\n",
    "res=df.na.fill('0',['city'])\n",
    "res.show()\n",
    "res.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4e7ebabf-1f6a-4842-a356-17d15990e946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+\n",
      "|Product|Amount|Country|\n",
      "+-------+------+-------+\n",
      "| Banana|  1000|    USA|\n",
      "|Carrots|  1500|    USA|\n",
      "|  Beans|  1600|    USA|\n",
      "| Orange|  2000|    USA|\n",
      "| Orange|  2000|    USA|\n",
      "| Banana|   400|  China|\n",
      "|Carrots|  1200|  China|\n",
      "|  Beans|  1500|  China|\n",
      "| Orange|  4000|  China|\n",
      "| Banana|  2000| Canada|\n",
      "|Carrots|  2000| Canada|\n",
      "|  Beans|  2000| Mexico|\n",
      "+-------+------+-------+\n",
      "\n",
      "+-------+------+-----+-------+------+\n",
      "|Country|Banana|Beans|Carrots|Orange|\n",
      "+-------+------+-----+-------+------+\n",
      "|  China|   400| 1500|   1200|  4000|\n",
      "|    USA|  1000| 1600|   1500|  4000|\n",
      "| Mexico|  NULL| 2000|   NULL|  NULL|\n",
      "| Canada|  2000| NULL|   2000|  NULL|\n",
      "+-------+------+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pivot\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('api').getOrCreate()\n",
    "data = [(\"Banana\",1000,\"USA\"), (\"Carrots\",1500,\"USA\"), (\"Beans\",1600,\"USA\"), \\\n",
    "      (\"Orange\",2000,\"USA\"),(\"Orange\",2000,\"USA\"),(\"Banana\",400,\"China\"), \\\n",
    "      (\"Carrots\",1200,\"China\"),(\"Beans\",1500,\"China\"),(\"Orange\",4000,\"China\"), \\\n",
    "      (\"Banana\",2000,\"Canada\"),(\"Carrots\",2000,\"Canada\"),(\"Beans\",2000,\"Mexico\")]\n",
    "\n",
    "col= [\"Product\",\"Amount\",\"Country\"]\n",
    "df=spark.createDataFrame(data,col)\n",
    "df.show()\n",
    "res=df.groupBy('Country').pivot('Product').sum('Amount')\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ca47648a-6b22-49ce-8f37-f72985df6d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+\n",
      "|Student|Subject|avg(Score)|\n",
      "+-------+-------+----------+\n",
      "|  Alice|   Math|      89.0|\n",
      "|    Bob|   Math|      85.0|\n",
      "|    Bob|Physics|      92.0|\n",
      "+-------+-------+----------+\n",
      "\n",
      "+-------+----+-------+\n",
      "|Student|Math|Physics|\n",
      "+-------+----+-------+\n",
      "|    Bob|85.0|   92.0|\n",
      "|  Alice|89.0|   NULL|\n",
      "+-------+----+-------+\n",
      "\n",
      "root\n",
      " |-- Student: string (nullable = true)\n",
      " |-- Math: double (nullable = true)\n",
      " |-- Physics: double (nullable = true)\n",
      "\n",
      "+-------+----+-------+\n",
      "|Student|Math|Physics|\n",
      "+-------+----+-------+\n",
      "|    Bob|85.0|   92.0|\n",
      "|  Alice|89.0|    0.0|\n",
      "+-------+----+-------+\n",
      "\n",
      "+-----------------------+\n",
      "|count(DISTINCT Subject)|\n",
      "+-----------------------+\n",
      "|                      2|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,countDistinct\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"PivotWithDuplicates\").getOrCreate()\n",
    "\n",
    "# Sample DataFrame with duplicates\n",
    "data = [(\"Alice\", \"Math\", 90),\n",
    "        (\"Bob\", \"Math\", 85),\n",
    "        (\"Alice\", \"Math\", 88),  # Duplicate entry for Alice and Math\n",
    "        (\"Bob\", \"Physics\", 92)]\n",
    "\n",
    "columns = [\"Student\", \"Subject\", \"Score\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Aggregation (e.g., taking the average score for duplicates)\n",
    "aggregated_df = df.groupBy(\"Student\", \"Subject\").agg({\"Score\": \"avg\"})\n",
    "aggregated_df.show()\n",
    "# Pivoting after aggregation\n",
    "pivot_df = aggregated_df.groupBy(\"Student\").pivot(\"Subject\").agg({\"avg(Score)\": \"first\"})\n",
    "\n",
    "# Show the pivoted DataFrame\n",
    "pivot_df.show()\n",
    "pivot_df.printSchema()\n",
    "res=pivot_df.fillna(0)\n",
    "res.show()\n",
    "res1=df.select(countDistinct('Subject'))\n",
    "res1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4915958f-ace6-4a0c-8a0b-5db09d8d6f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+----+\n",
      "|   name|age|     dept| sal|\n",
      "+-------+---+---------+----+\n",
      "|  Alice| 30|    Sales|5000|\n",
      "|    Bob| 25|Marketing|7000|\n",
      "|Charlie| 35|    Sales|6000|\n",
      "|  David| 28|Marketing|8000|\n",
      "+-------+---+---------+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', age=30, dept='Sales', sal=5000),\n",
       " Row(name='Bob', age=25, dept='Marketing', sal=7000),\n",
       " Row(name='Charlie', age=35, dept='Sales', sal=6000),\n",
       " Row(name='David', age=28, dept='Marketing', sal=8000)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import *\n",
    "\n",
    "spark=SparkSession.builder.appName('myapli').master('local[0]').getOrCreate()\n",
    "data=[(\"Alice\", 30, \"Sales\", 5000),\n",
    "        (\"Bob\", 25, \"Marketing\", 7000),\n",
    "        (\"Charlie\", 35, \"Sales\", 6000),\n",
    "        (\"David\", 28, \"Marketing\", 8000)]\n",
    "col=['name','age','dept','sal']\n",
    "df=spark.createDataFrame(data,col)\n",
    "df.show()\n",
    "res=df.rdd\n",
    "res.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1852c7be-2ee9-4bd4-b095-3bbf5001e6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|  Alice| 25|\n",
      "|  2|    Bob| 30|\n",
      "|  3|Charlie| 35|\n",
      "+---+-------+---+\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "SubqueryAlias man\n",
      "+- LogicalRDD [id#141, name#142, age#143], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: int, name: string, age: int\n",
      "SubqueryAlias man\n",
      "+- LogicalRDD [id#141, name#142, age#143], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "LogicalRDD [id#141, name#142, age#143], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Scan ExistingRDD[id#141,name#142,age#143]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"Dataset Example\").getOrCreate()\n",
    "\n",
    "# Define a schema for the dataset\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create a list of Row objects with the specified schema\n",
    "data = [\n",
    "    Row(id=1, name=\"Alice\", age=25),\n",
    "    Row(id=2, name=\"Bob\", age=30),\n",
    "    Row(id=3, name=\"Charlie\", age=35)\n",
    "]\n",
    "\n",
    "# Create a DataFrame from the list of Row objects and apply the schema\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Convert DataFrame to Dataset\n",
    "dataset = df.alias('man')\n",
    "\n",
    "# Perform transformations and actions on the Dataset\n",
    "# filtered_dataset = dataset.filter(dataset.age > 30)\n",
    "# selected_dataset = filtered_dataset.select(\"name\")\n",
    "\n",
    "# # Show the result\n",
    "# selected_dataset.show()\n",
    "\n",
    "# # Stop the SparkSession\n",
    "# spark.stop()\n",
    "dataset.show()\n",
    "dataset.explain('extended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88ad7444-b5a5-499d-9e64-d73069600c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|id   |gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|James    |          |Smith   |36636|M     |3000  |\n",
      "|Michael  |Rose      |        |40288|M     |4000  |\n",
      "|Robert   |          |Williams|42114|M     |4000  |\n",
      "|Maria    |Anne      |Jones   |39192|F     |4000  |\n",
      "|Jen      |Mary      |Brown   |     |F     |-1    |\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('SparkByExamples.com') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
    "  ]\n",
    "\n",
    "schema = StructType([ \n",
    "    StructField(\"firstname\",StringType(),True), \n",
    "    StructField(\"middlename\",StringType(),True), \n",
    "    StructField(\"lastname\",StringType(),True), \n",
    "    StructField(\"id\", StringType(), True), \n",
    "    StructField(\"gender\", StringType(), True), \n",
    "    StructField(\"salary\", IntegerType(), True) \n",
    "  ])\n",
    " \n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5cddf57a-dd42-45e7-83c6-764a40fd8080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+\n",
      "| name|age|gender|\n",
      "+-----+---+------+\n",
      "|  man| 23|     m|\n",
      "|  ram| 22|     m|\n",
      "|shami| 25|     f|\n",
      "+-----+---+------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n",
      "+-----+---+------+\n",
      "| name|age|gender|\n",
      "+-----+---+------+\n",
      "|  man| 23|     m|\n",
      "|  ram| 22|     m|\n",
      "|shami| 25|     f|\n",
      "+-----+---+------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType\n",
    "spark=SparkSession.builder.appName('apli').getOrCreate()\n",
    "data=[('man',23,'m'),\n",
    "      ('ram',22,'m'),\n",
    "      ('shami',25,'f')]\n",
    "\n",
    "schema=StructType([\n",
    "    StructField('name',StringType(),True),\n",
    "    StructField('age',IntegerType(),True),\n",
    "    StructField('gender',StringType(),True)])\n",
    "df=spark.createDataFrame(data,schema=schema)\n",
    "df.show()\n",
    "df.printSchema()\n",
    "# df=df.withColumn('age',df['age'].cast(StringType()))\n",
    "# df=df.withColumn('age',df['age'].astype(StringType()))\n",
    "df=df.withColumn('age',df['age'].cast('string'))\n",
    "df.show()\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "75eb7efd-a264-421c-a239-ceae8a0da59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- middle_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+----------+-----------+---------+-----+------+------+\n",
      "|first_name|middle_name|last_name|dob  |gender|salary|\n",
      "+----------+-----------+---------+-----+------+------+\n",
      "|James     |           |Smith    |36636|M     |60000 |\n",
      "|Michael   |Rose       |         |40288|M     |70000 |\n",
      "|Robert    |           |Williams |42114|      |400000|\n",
      "|Maria     |Anne       |Jones    |39192|F     |500000|\n",
      "|Jen       |Mary       |Brown    |     |F     |0     |\n",
      "+----------+-----------+---------+-----+------+------+\n",
      "\n",
      "  first_name middle_name last_name    dob gender  salary\n",
      "0      James                 Smith  36636      M   60000\n",
      "1    Michael        Rose            40288      M   70000\n",
      "2     Robert              Williams  42114         400000\n",
      "3      Maria        Anne     Jones  39192      F  500000\n",
      "4        Jen        Mary     Brown             F       0\n"
     ]
    }
   ],
   "source": [
    "#converting dataframe to pandas in pyspark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",60000),\n",
    "        (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",70000),\n",
    "        (\"Robert\",\"\",\"Williams\",\"42114\",\"\",400000),\n",
    "        (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",500000),\n",
    "        (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",0)]\n",
    "\n",
    "columns = [\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]\n",
    "pysparkDF = spark.createDataFrame(data = data, schema = columns)\n",
    "pysparkDF.printSchema()\n",
    "pysparkDF.show(truncate=False)\n",
    "res=pysparkDF.toPandas()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "be1f9477-7cb1-456c-bc0e-4b0430ec4c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+\n",
      "| id|name|age|\n",
      "+---+----+---+\n",
      "|  1| man| 23|\n",
      "|  2| ram| 24|\n",
      "|  3|sham| 25|\n",
      "+---+----+---+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from rdd to df\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('api').getOrCreate()\n",
    "data=[(1,'man',23),\n",
    "      (2,'ram',24),\n",
    "      (3,'sham',25)]\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "rdd.collect()\n",
    "df=rdd.toDF(['id','name','age'])\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b78fcfa5-8d58-487f-b4c7-f2d3fe1064ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|James| 23|\n",
      "|  Ann| 40|\n",
      "+-----+---+\n",
      "\n",
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|James| 23|\n",
      "|  Ann| 40|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('api').getOrCreate()\n",
    "data=[(\"James\",23),(\"Ann\",40)]\n",
    "col=['name','age']\n",
    "\n",
    "df=spark.createDataFrame(data,col).toDF('name','age')\n",
    "df1=spark.createDataFrame(data).toDF('name','age')\n",
    "\n",
    "df.show()\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2bca13b3-989f-4f34-8580-01bdc3bc6a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|prop.hair|\n",
      "+---------+\n",
      "|    black|\n",
      "|     blue|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "spark=SparkSession.builder.appName('api').getOrCreate()\n",
    "data=[Row(name='ram',prop=Row(hair='black',eye='blue')),\n",
    "      Row(name='sham',prop=Row(hair='blue',eye='green'))]\n",
    "df=spark.createDataFrame(data)\n",
    "df.select(df.prop.hair).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e15df715-79f6-49b1-9409-1d02188f899f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col1|col2|col3|\n",
      "+----+----+----+\n",
      "|   1|   2|   3|\n",
      "|   4|   5|   6|\n",
      "+----+----+----+\n",
      "\n",
      "+---+\n",
      "|sum|\n",
      "+---+\n",
      "|  3|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# todf and alias\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('api').getOrCreate()\n",
    "data=[(1,2,3),(4,5,6)]\n",
    "df=spark.createDataFrame(data).toDF('col1','col2','col3')\n",
    "df.show()\n",
    "df.select((df.col1+df.col2).alias('sum')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f4e04602-6639-47c4-afbd-8be13183c1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|     name|age|\n",
      "+---------+---+\n",
      "|James jos| 23|\n",
      "|      Ann| 40|\n",
      "+---------+---+\n",
      "\n",
      "+---------+---+\n",
      "|     name|age|\n",
      "+---------+---+\n",
      "|James jos| 23|\n",
      "+---------+---+\n",
      "\n",
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "| Ann| 40|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# contains and between\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('api').getOrCreate()\n",
    "data=[(\"James jos\",23),(\"Ann\",40)]\n",
    "df=spark.createDataFrame(data).toDF('name','age')\n",
    "df.show()\n",
    "res=df.filter(df.name.contains('jos'))\n",
    "res.show()\n",
    "df.where(df.age.between(30,45)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b4f4e434-6bdb-456f-95e0-5e4b5f0773ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|     name|age|\n",
      "+---------+---+\n",
      "|James jos| 23|\n",
      "|      Ann| 40|\n",
      "+---------+---+\n",
      "\n",
      "+---------+---+\n",
      "|     name|age|\n",
      "+---------+---+\n",
      "|James jos| 23|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# endswith\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('api').getOrCreate()\n",
    "data=[(\"James jos\",23),(\"Ann\",40),(\" sk\",19)]\n",
    "df=spark.createDataFrame(data).toDF('name','age')\n",
    "df.show()\n",
    "df.filter(df.name.endswith('jos')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d3a3fccb-90cc-4d4c-8135-6090ef15528e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "|fname|lname|age|\n",
      "+-----+-----+---+\n",
      "|James|  jos| 23|\n",
      "|  Ann|     | 40|\n",
      "|   sk|     | 19|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "|fname|lname|age|\n",
      "+-----+-----+---+\n",
      "|James|  jos| 23|\n",
      "|  Ann|     | 40|\n",
      "|   sk|     | 19|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# isnot null\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('api').getOrCreate()\n",
    "data=[(\"James\",\" jos\",23),(\"Ann\",\" \",40),(\"sk\",\" \",19)]\n",
    "df=spark.createDataFrame(data).toDF('fname','lname','age')\n",
    "df.show()\n",
    "df.filter(df.lname.isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9376121d-0e9c-47f1-926a-efab72c01e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "|fname|lname|age|\n",
      "+-----+-----+---+\n",
      "|James|  jos| 23|\n",
      "|  Ann|     | 40|\n",
      "|   sk|     | 19|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "|fname|lname|age|\n",
      "+-----+-----+---+\n",
      "|James|  jos| 23|\n",
      "|   sk|     | 19|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+\n",
      "|fname|lname|\n",
      "+-----+-----+\n",
      "|James|  jos|\n",
      "|  Ann|     |\n",
      "|   sk|     |\n",
      "+-----+-----+\n",
      "\n",
      "+-----+-----+---+\n",
      "|fname|lname|age|\n",
      "+-----+-----+---+\n",
      "|James|  jos| 23|\n",
      "|  Ann|     | 40|\n",
      "|   sk|     | 19|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('api').getOrCreate()\n",
    "data=[(\"James\",\" jos\",23),(\"Ann\",\" \",40),(\"sk\",\" \",19)]\n",
    "df=spark.createDataFrame(data).toDF('fname','lname','age')\n",
    "df.show()\n",
    "ag=[19,23]\n",
    "df.filter(df.age.isin(ag)).show()\n",
    "df.select(df.fname,df.lname).show()\n",
    "df.select('*').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d57e758a-a257-4b39-b7c8-c2607ffc6c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "|fname|lname|age|\n",
      "+-----+-----+---+\n",
      "|James|  jos| 23|\n",
      "|  Ann|     | 40|\n",
      "|   sk|     | 19|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+\n",
      "|fname|lname|\n",
      "+-----+-----+\n",
      "|James|  jos|\n",
      "|  Ann|     |\n",
      "+-----+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----+-----+---+-----------+\n",
      "|fname|lname|age|updated_age|\n",
      "+-----+-----+---+-----------+\n",
      "|James|  jos| 23|        230|\n",
      "|  Ann|     | 40|        400|\n",
      "|   sk|     | 19|        190|\n",
      "+-----+-----+---+-----------+\n",
      "\n",
      "+-----+-----+---+-------+\n",
      "|fname|lname|age|new_age|\n",
      "+-----+-----+---+-------+\n",
      "|James| jos |23 |230    |\n",
      "|Ann  |     |40 |400    |\n",
      "|sk   |     |19 |190    |\n",
      "+-----+-----+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "spark=SparkSession.builder.appName('api').getOrCreate()\n",
    "data=[(\"James\",\" jos\",23),(\"Ann\",\" \",40),(\"sk\",\" \",19)]\n",
    "df=spark.createDataFrame(data).toDF('fname','lname','age')\n",
    "df.show()\n",
    "df.select(df.columns[:2]).show(2)\n",
    "res=df.withColumn('updated_age',df.age*10)\n",
    "res.show()\n",
    "res1=res.withColumnRenamed('updated_age','new_age')\n",
    "res1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "95b34e61-29fa-46f2-81ab-47697d1b12fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "# Prepare Data\n",
    "data = [(\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600), \\\n",
    "    (\"Robert\", \"Sales\", 4100), \\\n",
    "    (\"Maria\", \"Finance\", 3000), \\\n",
    "    (\"James\", \"Sales\", 3000), \\\n",
    "    (\"Scott\", \"Finance\", 3300), \\\n",
    "    (\"Jen\", \"Finance\", 3900), \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000), \\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  ]\n",
    "\n",
    "# Create DataFrame\n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "res=df.distinct().count()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1a5c1fcc-8b64-4be2-8a16-917f6b9fc806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- zipcode: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- population: integer (nullable = true)\n",
      "\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|id |zipcode|type    |city               |state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|1  |704    |STANDARD|NULL               |PR   |30100     |\n",
      "|2  |704    |NULL    |PASEO COSTA DEL SUR|PR   |NULL      |\n",
      "|3  |709    |NULL    |BDA SAN LUIS       |PR   |3700      |\n",
      "|4  |76166  |UNIQUE  |CINGULAR WIRELESS  |TX   |84000     |\n",
      "|5  |76177  |STANDARD|NULL               |TX   |NULL      |\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|                  0|   PR|     30100|\n",
      "|  2|    704|    NULL|PASEO COSTA DEL SUR|   PR|      NULL|\n",
      "|  3|    709|    NULL|       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|STANDARD|                  0|   TX|      NULL|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"SparkByExamples.com\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "filePath=\"small_zipcode.csv\"\n",
    "df = spark.read.options(header='true', inferSchema='true') \\\n",
    "          .csv(filePath)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "df.na.fill('0','city').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e99217e7-bca9-45c6-87cc-78d6fea364c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+\n",
      "|   name|gender|salary|\n",
      "+-------+------+------+\n",
      "|  James|     M| 60000|\n",
      "|Michael|     M| 70000|\n",
      "| Robert|  NULL|400000|\n",
      "|  Maria|     F|500000|\n",
      "|    Jen|      |  NULL|\n",
      "+-------+------+------+\n",
      "\n",
      "+-------+------+------+----------+\n",
      "|   name|gender|salary|new_gender|\n",
      "+-------+------+------+----------+\n",
      "|  James|     M| 60000|      male|\n",
      "|Michael|     M| 70000|      male|\n",
      "| Robert|  NULL|400000|          |\n",
      "|  Maria|     F|500000|    female|\n",
      "|    Jen|      |  NULL|          |\n",
      "+-------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "data = [(\"James\",\"M\",60000),(\"Michael\",\"M\",70000),\n",
    "        (\"Robert\",None,400000),(\"Maria\",\"F\",500000),\n",
    "        (\"Jen\",\"\",None)]\n",
    "\n",
    "columns = [\"name\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.show()\n",
    "res=df.withColumn('new_gender',when(df.gender=='M','male')\n",
    "                              .when(df.gender=='F','female')\n",
    "                               .when(df.gender.isNull(),\"\")\n",
    "                               .otherwise(df.gender)\n",
    "                 )\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "5428a8ab-e509-4b40-8432-0d3e66dd44c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+\n",
      "|   name|gender|salary|\n",
      "+-------+------+------+\n",
      "|  James|     M| 60000|\n",
      "|Michael|     M| 70000|\n",
      "| Robert|  NULL|400000|\n",
      "|  Maria|     F|500000|\n",
      "|    Jen|      |  NULL|\n",
      "+-------+------+------+\n",
      "\n",
      "+-------+------+------+---------+\n",
      "|   name|gender|salary|newer_one|\n",
      "+-------+------+------+---------+\n",
      "|  James|     M| 60000|     male|\n",
      "|Michael|     M| 70000|     male|\n",
      "| Robert|  NULL|400000|         |\n",
      "|  Maria|     F|500000|   female|\n",
      "|    Jen|      |  NULL|         |\n",
      "+-------+------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when,col\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "data = [(\"James\",\"M\",60000),(\"Michael\",\"M\",70000),\n",
    "        (\"Robert\",None,400000),(\"Maria\",\"F\",500000),\n",
    "        (\"Jen\",\"\",None)]\n",
    "\n",
    "columns = [\"name\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.show()\n",
    "res=df.select(col('*'),when(df.gender=='M','male')\n",
    "                              .when(df.gender=='F','female')\n",
    "                               .when(df.gender.isNull(),\"\")\n",
    "                               .otherwise(df.gender).alias('newer_one')\n",
    "             )\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "3a6bace7-2062-4f3b-b7ab-ff390661cfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|gender|\n",
      "+-------+------+\n",
      "|  James|     M|\n",
      "|Michael|     F|\n",
      "|    Jen|      |\n",
      "+-------+------+\n",
      "\n",
      "+-------+------+----------+\n",
      "|   name|gender|new_gender|\n",
      "+-------+------+----------+\n",
      "|  James|     M|      male|\n",
      "|Michael|     F|    female|\n",
      "|    Jen|      |   unknown|\n",
      "+-------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('api').getOrCreate()\n",
    "data = [(\"James\",\"M\"),(\"Michael\",\"F\"),(\"Jen\",\"\")]\n",
    "columns = [\"name\",\"gender\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.show()\n",
    "res=df.withColumn('new_gender',expr(\"CASE WHEN gender=='M' THEN 'male' \"+\n",
    "                                     \"WHEN gender=='F' THEN 'female' ELSE 'unknown' END \"))\n",
    "\n",
    "res.show()\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "908af49a-de9c-4242-8e13-df20bab026bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+------+------+\n",
      "|                name|dob_year|gender|salary|\n",
      "+--------------------+--------+------+------+\n",
      "|     James, A, Smith|    2018|     M|  3000|\n",
      "|Michael, Rose, Jones|    2010|     M|  4000|\n",
      "|   Robert,K,Williams|    2010|     M|  4000|\n",
      "|    Maria,Anne,Jones|    2005|     F|  4000|\n",
      "|      Jen,Mary,Brown|    2010|      |    -1|\n",
      "+--------------------+--------+------+------+\n",
      "\n",
      "+------------------------+\n",
      "|new_name                |\n",
      "+------------------------+\n",
      "|[James,  A,  Smith]     |\n",
      "|[Michael,  Rose,  Jones]|\n",
      "|[Robert, K, Williams]   |\n",
      "|[Maria, Anne, Jones]    |\n",
      "|[Jen, Mary, Brown]      |\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "         .appName('SparkByExamples.com') \\\n",
    "         .getOrCreate()\n",
    "\n",
    "data = [(\"James, A, Smith\",\"2018\",\"M\",3000),\n",
    "            (\"Michael, Rose, Jones\",\"2010\",\"M\",4000),\n",
    "            (\"Robert,K,Williams\",\"2010\",\"M\",4000),\n",
    "            (\"Maria,Anne,Jones\",\"2005\",\"F\",4000),\n",
    "            (\"Jen,Mary,Brown\",\"2010\",\"\",-1)\n",
    "            ]\n",
    "\n",
    "columns=[\"name\",\"dob_year\",\"gender\",\"salary\"]\n",
    "df=spark.createDataFrame(data,columns)\n",
    "df.show()\n",
    "res=df.select(split(df.name,',').alias('new_name'))\n",
    "res.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "03d4c606-c87f-4460-aed0-c66135a0c515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----+\n",
      "| id|           address|state|\n",
      "+---+------------------+-----+\n",
      "|  1|           manmitj|   DE|\n",
      "|  2|43421 Margarita St|   NY|\n",
      "|  3|  13111 Siemon Ave|   CA|\n",
      "+---+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr,replace\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"ReplaceSubstringExample\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(1, \"14851 Jeffrey Road\", \"DE\"),\n",
    "        (2, \"43421 Margarita St\", \"NY\"),\n",
    "        (3, \"13111 Siemon Ave\", \"CA\")]\n",
    "\n",
    "# Create a DataFrame\n",
    "columns = [\"id\", \"address\", \"state\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "# res=df.replace('address','my_address')\n",
    "# res.show()\n",
    "# res=df.withColumn('address',when(endsWith((df.address),'Road'),'manmith').otherwise(df.address))\n",
    "res=df.replace('14851 Jeffrey Road','manmitj','address')\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d2d73f44-77c0-45dc-a249-7a2e13220358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "+---+------------------+-----+\n",
      "| id|           address|state|\n",
      "+---+------------------+-----+\n",
      "|  1|14851 Jeffrey Road|   DE|\n",
      "|  2|43421 Margarita St|   NY|\n",
      "|  3|  13111 Siemon Ave|   CA|\n",
      "+---+------------------+-----+\n",
      "\n",
      "\n",
      "DataFrame after replacing 'Road' with 'Manmith':\n",
      "+---+------------------+-----+\n",
      "| id|           address|state|\n",
      "+---+------------------+-----+\n",
      "|  1|           Manmith|   DE|\n",
      "|  2|43421 Margarita St|   NY|\n",
      "|  3|  13111 Siemon Ave|   CA|\n",
      "+---+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"ReplaceSubstringExample\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(1, \"14851 Jeffrey Road\", \"DE\"),\n",
    "        (2, \"43421 Margarita St\", \"NY\"),\n",
    "        (3, \"13111 Siemon Ave\", \"CA\")]\n",
    "\n",
    "# Create a DataFrame\n",
    "columns = [\"id\", \"address\", \"state\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "# Replace \"Road\" with \"Manmith\" in the \"address\" column using when and otherwise\n",
    "df_replaced = df.withColumn(\"address\", when(col(\"address\").contains(\"Road\"), \"Manmith\").otherwise(col(\"address\")))\n",
    "\n",
    "# Show the DataFrame after replacing the substring\n",
    "print(\"\\nDataFrame after replacing 'Road' with 'Manmith':\")\n",
    "df_replaced.show()\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "efa58020-9de7-4272-ab50-7752bc720e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------+-----+\n",
      "|id |address              |state|\n",
      "+---+---------------------+-----+\n",
      "|1  |14851 Jeffrey manmith|DE   |\n",
      "|2  |43421 Margarita St   |NY   |\n",
      "|3  |13111 Siemon Ave     |CA   |\n",
      "+---+---------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"ReplaceSubstringExample\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(1, \"14851 Jeffrey Road\", \"DE\"),\n",
    "        (2, \"43421 Margarita St\", \"NY\"),\n",
    "        (3, \"13111 Siemon Ave\", \"CA\")]\n",
    "\n",
    "# Create a DataFrame\n",
    "columns = [\"id\", \"address\", \"state\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "res=df.withColumn('address',regexp_replace('address','Road','manmith'))\n",
    "res.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "05263db1-e611-47e2-887d-d635e12ba948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+\n",
      "|   Name|     Dept|Salary|\n",
      "+-------+---------+------+\n",
      "|  James|    Sales|  3000|\n",
      "|Michael|    Sales|  4600|\n",
      "| Robert|    Sales|  4100|\n",
      "|  Maria|  Finance|  3000|\n",
      "|  James|    Sales|  3000|\n",
      "|  Scott|  Finance|  3300|\n",
      "|    Jen|  Finance|  3900|\n",
      "|   Jeff|Marketing|  3000|\n",
      "|  Kumar|Marketing|  2000|\n",
      "|   Saif|    Sales|  4100|\n",
      "+-------+---------+------+\n",
      "\n",
      "+--------+-------------+\n",
      "|count(1)|distinctcount|\n",
      "+--------+-------------+\n",
      "|      10|            9|\n",
      "+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import countDistinct,count\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "         .appName('SparkByExamples.com') \\\n",
    "         .getOrCreate()\n",
    "\n",
    "data = [(\"James\", \"Sales\", 3000),\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000),\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Scott\", \"Finance\", 3300),\n",
    "    (\"Jen\", \"Finance\", 3900),\n",
    "    (\"Jeff\", \"Marketing\", 3000),\n",
    "    (\"Kumar\", \"Marketing\", 2000),\n",
    "    (\"Saif\", \"Sales\", 4100)\n",
    "  ]\n",
    "columns = [\"Name\",\"Dept\",\"Salary\"]\n",
    "df=spark.createDataFrame(data,columns)\n",
    "df.show()\n",
    "res=df.distinct()\n",
    "res.count()\n",
    "res1=df.select(count('*'),countDistinct('*').alias('distinctcount'))\n",
    "res1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "52c5370e-24b6-4169-9f6f-015f5809292a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (994281860.py, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[201], line 20\u001b[0;36m\u001b[0m\n\u001b[0;31m    dataset = spark.createDataFrame(data, [\"id\", \"name\", \"age\"]).as[Person]\u001b[0m\n\u001b[0m                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"DatasetExample\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(1, \"Alice\", 25),\n",
    "        (2, \"Bob\", 30),\n",
    "        (3, \"Charlie\", 35)]\n",
    "\n",
    "# Define a case class representing the schema\n",
    "class Person:\n",
    "    def __init__(self, id, name, age):\n",
    "        self.id = id\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "\n",
    "# Create a Dataset from the sample data and the case class\n",
    "dataset = spark.createDataFrame(data, [\"id\", \"name\", \"age\"]).as[Person]\n",
    "\n",
    "# Show the original Dataset\n",
    "print(\"Original Dataset:\")\n",
    "dataset.show()\n",
    "\n",
    "# Filter the Dataset to select persons with age greater than 30\n",
    "filtered_dataset = dataset.filter(col(\"age\") > 30)\n",
    "\n",
    "# Show the filtered Dataset\n",
    "print(\"\\nFiltered Dataset (persons with age > 30):\")\n",
    "filtered_dataset.show()\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c1890a6-5915-4112-890b-691d138949ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "+---------+--------+---+\n",
      "|FirstName|LastName|Age|\n",
      "+---------+--------+---+\n",
      "|     John|     Doe| 28|\n",
      "|     Jane|     Doe| 22|\n",
      "|      Bob|   Smith| 35|\n",
      "|     Jane|     Doe| 22|\n",
      "|     John|     Doe| 28|\n",
      "+---------+--------+---+\n",
      "\n",
      "+---------+--------+---+\n",
      "|FirstName|LastName|Age|\n",
      "+---------+--------+---+\n",
      "|     John|     Doe| 28|\n",
      "|     Jane|     Doe| 22|\n",
      "|      Bob|   Smith| 35|\n",
      "+---------+--------+---+\n",
      "\n",
      "+---------+-------+---+\n",
      "|FirstName|manmith|Age|\n",
      "+---------+-------+---+\n",
      "|     John|    Doe| 28|\n",
      "|     Jane|    Doe| 22|\n",
      "|      Bob|  Smith| 35|\n",
      "+---------+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Sample data with duplicates\n",
    "data = [(\"John\", \"Doe\", 28),\n",
    "        (\"Jane\", \"Doe\", 22),\n",
    "        (\"Bob\", \"Smith\", 35),\n",
    "        (\"Jane\", \"Doe\", 22),\n",
    "        (\"John\", \"Doe\", 28)]\n",
    "\n",
    "# Define schema\n",
    "columns = [\"FirstName\", \"LastName\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "df.show()\n",
    "# res=df.drop_duplicates()\n",
    "res=df.dropDuplicates()\n",
    "res.show()\n",
    "res1=res.select('FirstName',df.LastName.alias('manmith'),'Age')\n",
    "res1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a39f6be-2711-43b4-9d35-12dba8dec72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "||\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import min as pyspark_min, max as pyspark_max\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [(\"Alice\", 25, 160.5),\n",
    "        (\"Bob\", 30, 175.2),\n",
    "        (\"Charlie\", 35, 180.0)]\n",
    "\n",
    "columns = [\"Name\", \"Age\", \"Height\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Identify integer columns\n",
    "integer_columns = [col_name for col_name, col_type in df.dtypes if isinstance(df[col_name].schema, IntegerType)]\n",
    "\n",
    "# Perform min and max operations for each integer column\n",
    "min_max_operations = {col_name: [pyspark_min(col_name).alias(\"min\"), pyspark_max(col_name).alias(\"max\")] for col_name in integer_columns}\n",
    "\n",
    "# Apply the aggregation\n",
    "result = df.agg(min_max_operations)\n",
    "\n",
    "# Show the result\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dda5554-921a-4426-a24e-add5089789e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
